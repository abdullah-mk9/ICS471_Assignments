{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A4Q6DBB3FVY"
      },
      "source": [
        "# Neural Network Implementation-From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ7Bd9ZsoXCR"
      },
      "source": [
        "## A. Neural Network: Binary Classification \n",
        "\n",
        "In this assignment, we will learn to build a fully-connected neural network with standard architecture, i.e., only one hidden layer. We will use the `Breast cancer wisconsin (diagnostic) dataset` available in `https://scikit-learn.org/stable/datasets/index.html`. It has a total of 569 sample with two classes (Malignant and Benign). Each sample has 30 real-valued features. \n",
        "\n",
        "**This assignment will help you understand and implement:**\n",
        "- A neural network for binary classification consisting of one hidden layer with non-linear activation. You will implement ideas like forward propogation, computing the loss, bagward propogation, and parameter(weights) update. Using the trained network, you will learn to predict a class given the features of a sample.\n",
        "\n",
        "**Note:** There are multiple conventions for coding neural networks. We will follow the conventions suggested by Andrew Ng: https://www.coursera.org/learn/neural-networks-deep-learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "sqaGHZSJoXCU"
      },
      "outputs": [],
      "source": [
        "# Package imports\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.linear_model\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGkbrWCS3FVm"
      },
      "source": [
        "## 1. Loading the dataset and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "LENi5Q9Z3FVn"
      },
      "outputs": [],
      "source": [
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size =0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "LkzSuMSr3FVo"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reshaping the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rpzj7FmV3FVq"
      },
      "outputs": [],
      "source": [
        "trainx = train_data.T\n",
        "trainy = train_labels.reshape(-1,1).T\n",
        "\n",
        "testx = test_data.T\n",
        "testy =test_labels.reshape(-1,1).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "d78k8Prk3FVs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((30, 455), (1, 455), (30, 114), (1, 114))"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainx.shape, trainy.shape, testx.shape, testy.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "0dIZ3-k23FVu"
      },
      "outputs": [],
      "source": [
        "X=trainx\n",
        "Y=trainy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "DFAQt6HYoXCh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of training samples: 455\n",
            "Number of features per sample: 30\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "shape_X = X.shape\n",
        "shape_Y = Y.shape\n",
        "m = shape_X[1]  # training set size\n",
        "### END CODE HERE ###\n",
        "\n",
        "print ('No. of training samples: ' + str(m))\n",
        "print ('Number of features per sample: ' + str(shape_X[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2AFzm-hoXCv"
      },
      "source": [
        "## 2 - Neural Network model\n",
        "\n",
        "We will train a Neural Network with a single hidden layer.\n",
        "\n",
        "**Mathematically**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
        "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
        "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
        "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
        "$$y^{(i)}_{prediction} = \\begin{cases} 1 & {if } a^{[2](i)} > 0.5 \\\\ 0 & {otherwise } \\end{cases}\\tag{5}$$\n",
        "\n",
        "Given the predictions on all the examples, you can also compute the cost (loss) $J$ as follows: \n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
        "\n",
        "**Important**: Building the NN will involve the following:\n",
        "\n",
        "    1. Specify the network structure in terms of the number of input units, number of neurons in the hidden units, ...\n",
        "    2. Initialize the parameters of the model\n",
        "    3. Loop a number of iterations:\n",
        "        - Forward propagation\n",
        "        - Compute loss and the overall loss\n",
        "        - Backward propagation\n",
        "        - Update the parameters (gradient descent)\n",
        "\n",
        "In order to make the code modular, we can implement each of the step as a function and them combine them together to build the overall model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC3UrB-AoXCw"
      },
      "source": [
        "### 2.1 - Specify the network structure \n",
        "    - n_x: input layer size\n",
        "    - n_h: #neurons in  hidden layer (hard code a value, say 10) \n",
        "    - n_y: the size of the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "mOsdX_bPoXCx"
      },
      "outputs": [],
      "source": [
        "def model_architecture(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### \n",
        "    n_x = X.shape[0] # size of input layer\n",
        "    n_h = 10\n",
        "    n_y = len(np.unique(Y)) - 1 # number ofsize of output layer\n",
        "    ### END CODE HERE ###\n",
        "    return (n_x, n_h, n_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4-FAtl-oXC3"
      },
      "source": [
        "### 2.2 - Initialize the parameters of the model\n",
        "\n",
        "- Initialize the weights matrices with random values. \n",
        "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
        "- Initialize the bias vectors as zeros. \n",
        "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "XV3W6uLvoXC3"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(2)\n",
        "\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
        "    b1 = np.zeros((n_h,1))\n",
        "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
        "    b2 = np.zeros((n_y,1))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBRbnqYkoXC8"
      },
      "source": [
        "### 2.3 - The Loop ####\n",
        "\n",
        "**Instructions**:\n",
        "- Look above at the mathematical representation of your classifier.\n",
        "- Define the function `sigmoid()`.\n",
        "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
        "- The steps you have to implement are:\n",
        "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
        "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
        "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "-4Xif_6C3FV3"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    ### Update THE CODE HERE ###\n",
        "    x = 1/(1+np.exp(-x))\n",
        "    return x\n",
        "    ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "AkJ4TKKSoXC9"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(X, parameters, a1=np.tanh, a2= sigmoid):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    ### START CODE HERE ### \n",
        "    Z1 = W1 @ X\n",
        "    A1 = a1(Z1)\n",
        "    Z2 = W2 @ A1\n",
        "    A2 = a2(Z2)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21VnrjU3oXDC"
      },
      "source": [
        "Now you have computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for every example, you can compute the loss function as follows:\n",
        "\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{7}$$\n",
        "\n",
        "- Implement the cross-entropy loss:\n",
        "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
        "```python\n",
        "logprobs = np.multiply(np.log(A2),Y)\n",
        "loss = - np.sum(logprobs)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_loss(A2,Y):\n",
        "    logprobs = np.multiply(np.log(A2),Y)\n",
        "    loss = -1 * np.sum(logprobs)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "whxXuxq-oXDD"
      },
      "outputs": [],
      "source": [
        "def compute_loss(A2, Y, loss_function = cross_entropy_loss):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "       \n",
        "    Returns:\n",
        "    loss -- cross-entropy loss given equation (7)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1] # number of example\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    ### START CODE HERE ###\n",
        "    loss = loss_function(A2,Y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    loss = float(np.squeeze(loss))\n",
        "\n",
        "    assert(isinstance(loss, float))\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5puT2VVoXDH"
      },
      "source": [
        "Using the cache computed during forward propagation, we can now implement backward propagation.\n",
        "\n",
        "Backpropagation is usually the hardest (most mathematical) part. You can use the following six equations as vectorized implementation:\n",
        "\n",
        "$$dZ^{[2]} = A^{[2]} - Y \\tag{8}$$ \n",
        "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]}A^{[1]{T}} \\tag{9}$$ \n",
        "$$db^{[2]} = \\frac{1}{m} np.sum(dZ^{[2]}, axis = 1, keepdims = True)\\tag{10}$$ \n",
        "$$dZ^{[1]} = W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]}) \\tag{11}$$ \n",
        "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]}X^{{T}} \\tag{12}$$ \n",
        "$$db^{[1]} = \\frac{1}{m} np.sum(dZ^{[1]}, axis = 1, keepdims = True)\\tag{13}$$ \n",
        "\n",
        "- $*$ denotes elementwise multiplication.\n",
        "- Notations followed:\n",
        "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
        "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
        "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
        "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
        "   \n",
        "- Tips:\n",
        "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
        "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "KHTVtf4_oXDJ"
      },
      "outputs": [],
      "source": [
        "def backprop(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data\n",
        "    Y -- \"true\" labels\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    ### START CODE HERE ### \n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    ### START CODE HERE ### \n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = ((1/m)* dZ2) @ A1.T\n",
        "    db2 = (1/m)* np.sum(dZ2, axis=1, keepdims=True)\n",
        "    #possible problem\n",
        "    dZ1 = ( W2.T @ dZ2 ) * (1 - np.power(A1,2))\n",
        "    dW1 = (1/m)*dZ1@ X.T\n",
        "    db1 = (1/m)* np.sum(dZ1, axis=1, keepdims=True)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsDhe51IoXDO"
      },
      "source": [
        "Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
        "\n",
        "**Gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "xWeErPRmoXDP"
      },
      "outputs": [],
      "source": [
        "def update(parameters, grads, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    learning_rate -- The learning rate\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    ### START CODE HERE ### \n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "    ## END CODE HERE ###\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    ### START CODE HERE ### \n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSWq5OE4oXDT"
      },
      "source": [
        "### 2.4 - Integrate parts 2.1, 2.2 and 2.3 in NeuralNetwork() ####\n",
        "\n",
        "Build your neural network model in `NeuralNetwork()`.\n",
        "\n",
        "**Instructions**: The neural network model has to use the previous functions in the right order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "cnSyk2auoXDU"
      },
      "outputs": [],
      "source": [
        "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_loss=False, a1=np.tanh, a2=sigmoid, loss_function = cross_entropy_loss):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset\n",
        "    Y -- labels \n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    learning_rate -- The learning rate\n",
        "    print_loss -- if True, print the loss every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    n_x = model_architecture(X, Y)[0]\n",
        "    n_y = model_architecture(X, Y)[2]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### \n",
        "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        ### START CODE HERE ### \n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X, parameters, a1=a1, a2= a2)\n",
        "        \n",
        "        # loss function. Inputs: \"A2, Y, parameters\". Outputs: \"loss\".\n",
        "        loss = compute_loss(A2,Y, loss_function = loss_function)\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backprop(parameters,cache,X,Y)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters =  update(parameters,grads)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Print the loss every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            print (\"loss after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ud31U_ZoXDY"
      },
      "source": [
        "### 2.5 Predictions\n",
        "\n",
        "Use your model to predict by building predict().\n",
        "\n",
        "**Reminder**: $ predictions = \\begin{cases}\n",
        "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}$  \n",
        "    \n",
        "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "YPMG_zMloXDZ"
      },
      "outputs": [],
      "source": [
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data \n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ### \n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = np.where(A2> 0.5, 1 , 0)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC3OHe-9oXDd"
      },
      "source": [
        "## 3. Model Execution\n",
        "It is time to run the model and see how it performs on the dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "op5NM0gXoXDi",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss after iteration 0: 200.949495\n",
            "loss after iteration 100: 199.343186\n",
            "loss after iteration 200: 185.530896\n",
            "loss after iteration 300: 148.719538\n",
            "loss after iteration 400: 110.707548\n",
            "loss after iteration 500: 83.261527\n",
            "loss after iteration 600: 65.143750\n",
            "loss after iteration 700: 53.196355\n",
            "loss after iteration 800: 44.957613\n",
            "loss after iteration 900: 39.013066\n",
            "loss after iteration 1000: 34.557922\n",
            "loss after iteration 1100: 31.111772\n",
            "loss after iteration 1200: 28.374280\n",
            "loss after iteration 1300: 26.150190\n",
            "loss after iteration 1400: 24.308265\n",
            "loss after iteration 1500: 22.757701\n",
            "loss after iteration 1600: 21.434017\n",
            "loss after iteration 1700: 20.290334\n",
            "loss after iteration 1800: 19.291838\n",
            "loss after iteration 1900: 18.412169\n",
            "loss after iteration 2000: 17.631012\n",
            "loss after iteration 2100: 16.932463\n",
            "loss after iteration 2200: 16.303890\n",
            "loss after iteration 2300: 15.735130\n",
            "loss after iteration 2400: 15.217910\n",
            "loss after iteration 2500: 14.745427\n",
            "loss after iteration 2600: 14.312030\n",
            "loss after iteration 2700: 13.912989\n",
            "loss after iteration 2800: 13.544310\n",
            "loss after iteration 2900: 13.202599\n",
            "loss after iteration 3000: 12.884952\n",
            "loss after iteration 3100: 12.588869\n",
            "loss after iteration 3200: 12.312187\n",
            "loss after iteration 3300: 12.053026\n",
            "loss after iteration 3400: 11.809741\n",
            "loss after iteration 3500: 11.580889\n",
            "loss after iteration 3600: 11.365197\n",
            "loss after iteration 3700: 11.161541\n",
            "loss after iteration 3800: 10.968919\n",
            "loss after iteration 3900: 10.786440\n",
            "loss after iteration 4000: 10.613306\n",
            "loss after iteration 4100: 10.448802\n",
            "loss after iteration 4200: 10.292285\n",
            "loss after iteration 4300: 10.143172\n",
            "loss after iteration 4400: 10.000939\n",
            "loss after iteration 4500: 9.865108\n",
            "loss after iteration 4600: 9.735248\n",
            "loss after iteration 4700: 9.610964\n",
            "loss after iteration 4800: 9.491898\n",
            "loss after iteration 4900: 9.377725\n",
            "loss after iteration 5000: 9.268146\n",
            "loss after iteration 5100: 9.162891\n",
            "loss after iteration 5200: 9.061712\n",
            "loss after iteration 5300: 8.964386\n",
            "loss after iteration 5400: 8.870708\n",
            "loss after iteration 5500: 8.780493\n",
            "loss after iteration 5600: 8.693574\n",
            "loss after iteration 5700: 8.609796\n",
            "loss after iteration 5800: 8.529023\n",
            "loss after iteration 5900: 8.451127\n",
            "loss after iteration 6000: 8.375994\n",
            "loss after iteration 6100: 8.303517\n",
            "loss after iteration 6200: 8.233598\n",
            "loss after iteration 6300: 8.166146\n",
            "loss after iteration 6400: 8.101075\n",
            "loss after iteration 6500: 8.038304\n",
            "loss after iteration 6600: 7.977755\n",
            "loss after iteration 6700: 7.919351\n",
            "loss after iteration 6800: 7.863019\n",
            "loss after iteration 6900: 7.808685\n",
            "loss after iteration 7000: 7.756278\n",
            "loss after iteration 7100: 7.705726\n",
            "loss after iteration 7200: 7.656955\n",
            "loss after iteration 7300: 7.609896\n",
            "loss after iteration 7400: 7.564477\n",
            "loss after iteration 7500: 7.520626\n",
            "loss after iteration 7600: 7.478274\n",
            "loss after iteration 7700: 7.437350\n",
            "loss after iteration 7800: 7.397786\n",
            "loss after iteration 7900: 7.359516\n",
            "loss after iteration 8000: 7.322474\n",
            "loss after iteration 8100: 7.286598\n",
            "loss after iteration 8200: 7.251828\n",
            "loss after iteration 8300: 7.218107\n",
            "loss after iteration 8400: 7.185379\n",
            "loss after iteration 8500: 7.153594\n",
            "loss after iteration 8600: 7.122704\n",
            "loss after iteration 8700: 7.092662\n",
            "loss after iteration 8800: 7.063426\n",
            "loss after iteration 8900: 7.034958\n",
            "loss after iteration 9000: 7.007221\n",
            "loss after iteration 9100: 6.980181\n",
            "loss after iteration 9200: 6.953808\n",
            "loss after iteration 9300: 6.928072\n",
            "loss after iteration 9400: 6.902948\n",
            "loss after iteration 9500: 6.878411\n",
            "loss after iteration 9600: 6.854438\n",
            "loss after iteration 9700: 6.831008\n",
            "loss after iteration 9800: 6.808103\n",
            "loss after iteration 9900: 6.785704\n",
            "loss after iteration 10000: 6.763796\n",
            "loss after iteration 10100: 6.742361\n",
            "loss after iteration 10200: 6.721387\n",
            "loss after iteration 10300: 6.700858\n",
            "loss after iteration 10400: 6.680764\n",
            "loss after iteration 10500: 6.661091\n",
            "loss after iteration 10600: 6.641828\n",
            "loss after iteration 10700: 6.622965\n",
            "loss after iteration 10800: 6.604492\n",
            "loss after iteration 10900: 6.586399\n",
            "loss after iteration 11000: 6.568676\n",
            "loss after iteration 11100: 6.551316\n",
            "loss after iteration 11200: 6.534310\n",
            "loss after iteration 11300: 6.517651\n",
            "loss after iteration 11400: 6.501331\n",
            "loss after iteration 11500: 6.485344\n",
            "loss after iteration 11600: 6.469682\n",
            "loss after iteration 11700: 6.454341\n",
            "loss after iteration 11800: 6.439314\n",
            "loss after iteration 11900: 6.424598\n",
            "loss after iteration 12000: 6.410186\n",
            "loss after iteration 12100: 6.396076\n",
            "loss after iteration 12200: 6.382263\n",
            "loss after iteration 12300: 6.368746\n",
            "loss after iteration 12400: 6.355521\n",
            "loss after iteration 12500: 6.342588\n",
            "loss after iteration 12600: 6.329946\n",
            "loss after iteration 12700: 6.317595\n",
            "loss after iteration 12800: 6.305536\n",
            "loss after iteration 12900: 6.293772\n",
            "loss after iteration 13000: 6.282305\n",
            "loss after iteration 13100: 6.271140\n",
            "loss after iteration 13200: 6.260283\n",
            "loss after iteration 13300: 6.249741\n",
            "loss after iteration 13400: 6.239524\n",
            "loss after iteration 13500: 6.229642\n",
            "loss after iteration 13600: 6.220106\n",
            "loss after iteration 13700: 6.210932\n",
            "loss after iteration 13800: 6.202134\n",
            "loss after iteration 13900: 6.193730\n",
            "loss after iteration 14000: 6.185738\n",
            "loss after iteration 14100: 6.178174\n",
            "loss after iteration 14200: 6.171055\n",
            "loss after iteration 14300: 6.164396\n",
            "loss after iteration 14400: 6.158205\n",
            "loss after iteration 14500: 6.152487\n",
            "loss after iteration 14600: 6.147234\n",
            "loss after iteration 14700: 6.142430\n",
            "loss after iteration 14800: 6.138043\n",
            "loss after iteration 14900: 6.134027\n",
            "loss after iteration 15000: 6.130323\n",
            "loss after iteration 15100: 6.126856\n",
            "loss after iteration 15200: 6.123543\n",
            "loss after iteration 15300: 6.120292\n",
            "loss after iteration 15400: 6.117007\n",
            "loss after iteration 15500: 6.113596\n",
            "loss after iteration 15600: 6.109972\n",
            "loss after iteration 15700: 6.106057\n",
            "loss after iteration 15800: 6.101787\n",
            "loss after iteration 15900: 6.097109\n",
            "loss after iteration 16000: 6.091985\n",
            "loss after iteration 16100: 6.086393\n",
            "loss after iteration 16200: 6.080321\n",
            "loss after iteration 16300: 6.073769\n",
            "loss after iteration 16400: 6.066747\n",
            "loss after iteration 16500: 6.059271\n",
            "loss after iteration 16600: 6.051366\n",
            "loss after iteration 16700: 6.043057\n",
            "loss after iteration 16800: 6.034374\n",
            "loss after iteration 16900: 6.025348\n",
            "loss after iteration 17000: 6.016012\n",
            "loss after iteration 17100: 6.006395\n",
            "loss after iteration 17200: 5.996529\n",
            "loss after iteration 17300: 5.986441\n",
            "loss after iteration 17400: 5.976159\n",
            "loss after iteration 17500: 5.965708\n",
            "loss after iteration 17600: 5.955111\n",
            "loss after iteration 17700: 5.944389\n",
            "loss after iteration 17800: 5.933561\n",
            "loss after iteration 17900: 5.922645\n",
            "loss after iteration 18000: 5.911656\n",
            "loss after iteration 18100: 5.900608\n",
            "loss after iteration 18200: 5.889514\n",
            "loss after iteration 18300: 5.878384\n",
            "loss after iteration 18400: 5.867228\n",
            "loss after iteration 18500: 5.856055\n",
            "loss after iteration 18600: 5.844871\n",
            "loss after iteration 18700: 5.833685\n",
            "loss after iteration 18800: 5.822500\n",
            "loss after iteration 18900: 5.811323\n",
            "loss after iteration 19000: 5.800158\n",
            "loss after iteration 19100: 5.789008\n",
            "loss after iteration 19200: 5.777877\n",
            "loss after iteration 19300: 5.766768\n",
            "loss after iteration 19400: 5.755683\n",
            "loss after iteration 19500: 5.744624\n",
            "loss after iteration 19600: 5.733593\n",
            "loss after iteration 19700: 5.722591\n",
            "loss after iteration 19800: 5.711621\n",
            "loss after iteration 19900: 5.700683\n",
            "loss after iteration 20000: 5.689778\n",
            "loss after iteration 20100: 5.678906\n",
            "loss after iteration 20200: 5.668070\n",
            "loss after iteration 20300: 5.657269\n",
            "loss after iteration 20400: 5.646504\n",
            "loss after iteration 20500: 5.635774\n",
            "loss after iteration 20600: 5.625082\n",
            "loss after iteration 20700: 5.614426\n",
            "loss after iteration 20800: 5.603808\n",
            "loss after iteration 20900: 5.593227\n",
            "loss after iteration 21000: 5.582683\n",
            "loss after iteration 21100: 5.572177\n",
            "loss after iteration 21200: 5.561708\n",
            "loss after iteration 21300: 5.551277\n",
            "loss after iteration 21400: 5.540884\n",
            "loss after iteration 21500: 5.530528\n",
            "loss after iteration 21600: 5.520210\n",
            "loss after iteration 21700: 5.509930\n",
            "loss after iteration 21800: 5.499687\n",
            "loss after iteration 21900: 5.489481\n",
            "loss after iteration 22000: 5.479312\n",
            "loss after iteration 22100: 5.469181\n",
            "loss after iteration 22200: 5.459087\n",
            "loss after iteration 22300: 5.449029\n",
            "loss after iteration 22400: 5.439008\n",
            "loss after iteration 22500: 5.429024\n",
            "loss after iteration 22600: 5.419075\n",
            "loss after iteration 22700: 5.409163\n",
            "loss after iteration 22800: 5.399287\n",
            "loss after iteration 22900: 5.389446\n",
            "loss after iteration 23000: 5.379641\n",
            "loss after iteration 23100: 5.369871\n",
            "loss after iteration 23200: 5.360136\n",
            "loss after iteration 23300: 5.350436\n",
            "loss after iteration 23400: 5.340770\n",
            "loss after iteration 23500: 5.331138\n",
            "loss after iteration 23600: 5.321540\n",
            "loss after iteration 23700: 5.311976\n",
            "loss after iteration 23800: 5.302445\n",
            "loss after iteration 23900: 5.292947\n",
            "loss after iteration 24000: 5.283482\n",
            "loss after iteration 24100: 5.274049\n",
            "loss after iteration 24200: 5.264649\n",
            "loss after iteration 24300: 5.255281\n",
            "loss after iteration 24400: 5.245944\n",
            "loss after iteration 24500: 5.236638\n",
            "loss after iteration 24600: 5.227364\n",
            "loss after iteration 24700: 5.218120\n",
            "loss after iteration 24800: 5.208906\n",
            "loss after iteration 24900: 5.199722\n",
            "loss after iteration 25000: 5.190568\n",
            "loss after iteration 25100: 5.181444\n",
            "loss after iteration 25200: 5.172348\n",
            "loss after iteration 25300: 5.163281\n",
            "loss after iteration 25400: 5.154242\n",
            "loss after iteration 25500: 5.145232\n",
            "loss after iteration 25600: 5.136248\n",
            "loss after iteration 25700: 5.127293\n",
            "loss after iteration 25800: 5.118363\n",
            "loss after iteration 25900: 5.109461\n",
            "loss after iteration 26000: 5.100584\n",
            "loss after iteration 26100: 5.091732\n",
            "loss after iteration 26200: 5.082906\n",
            "loss after iteration 26300: 5.074104\n",
            "loss after iteration 26400: 5.065326\n",
            "loss after iteration 26500: 5.056572\n",
            "loss after iteration 26600: 5.047840\n",
            "loss after iteration 26700: 5.039130\n",
            "loss after iteration 26800: 5.030442\n",
            "loss after iteration 26900: 5.021775\n",
            "loss after iteration 27000: 5.013127\n",
            "loss after iteration 27100: 5.004498\n",
            "loss after iteration 27200: 4.995888\n",
            "loss after iteration 27300: 4.987295\n",
            "loss after iteration 27400: 4.978718\n",
            "loss after iteration 27500: 4.970156\n",
            "loss after iteration 27600: 4.961607\n",
            "loss after iteration 27700: 4.953072\n",
            "loss after iteration 27800: 4.944548\n",
            "loss after iteration 27900: 4.936035\n",
            "loss after iteration 28000: 4.927530\n",
            "loss after iteration 28100: 4.919033\n",
            "loss after iteration 28200: 4.910542\n",
            "loss after iteration 28300: 4.902057\n",
            "loss after iteration 28400: 4.893575\n",
            "loss after iteration 28500: 4.885095\n",
            "loss after iteration 28600: 4.876617\n",
            "loss after iteration 28700: 4.868138\n",
            "loss after iteration 28800: 4.859658\n",
            "loss after iteration 28900: 4.851176\n",
            "loss after iteration 29000: 4.842690\n",
            "loss after iteration 29100: 4.834200\n",
            "loss after iteration 29200: 4.825705\n",
            "loss after iteration 29300: 4.817203\n",
            "loss after iteration 29400: 4.808694\n",
            "loss after iteration 29500: 4.800177\n",
            "loss after iteration 29600: 4.791653\n",
            "loss after iteration 29700: 4.783119\n",
            "loss after iteration 29800: 4.774576\n",
            "loss after iteration 29900: 4.766023\n",
            "loss after iteration 30000: 4.757461\n",
            "loss after iteration 30100: 4.748888\n",
            "loss after iteration 30200: 4.740304\n",
            "loss after iteration 30300: 4.731709\n",
            "loss after iteration 30400: 4.723104\n",
            "loss after iteration 30500: 4.714488\n",
            "loss after iteration 30600: 4.705860\n",
            "loss after iteration 30700: 4.697221\n",
            "loss after iteration 30800: 4.688571\n",
            "loss after iteration 30900: 4.679910\n",
            "loss after iteration 31000: 4.671238\n",
            "loss after iteration 31100: 4.662554\n",
            "loss after iteration 31200: 4.653859\n",
            "loss after iteration 31300: 4.645154\n",
            "loss after iteration 31400: 4.636438\n",
            "loss after iteration 31500: 4.627711\n",
            "loss after iteration 31600: 4.618973\n",
            "loss after iteration 31700: 4.610225\n",
            "loss after iteration 31800: 4.601467\n",
            "loss after iteration 31900: 4.592700\n",
            "loss after iteration 32000: 4.583922\n",
            "loss after iteration 32100: 4.575135\n",
            "loss after iteration 32200: 4.566339\n",
            "loss after iteration 32300: 4.557535\n",
            "loss after iteration 32400: 4.548722\n",
            "loss after iteration 32500: 4.539901\n",
            "loss after iteration 32600: 4.531072\n",
            "loss after iteration 32700: 4.522237\n",
            "loss after iteration 32800: 4.513394\n",
            "loss after iteration 32900: 4.504546\n",
            "loss after iteration 33000: 4.495692\n",
            "loss after iteration 33100: 4.486832\n",
            "loss after iteration 33200: 4.477969\n",
            "loss after iteration 33300: 4.469101\n",
            "loss after iteration 33400: 4.460230\n",
            "loss after iteration 33500: 4.451356\n",
            "loss after iteration 33600: 4.442480\n",
            "loss after iteration 33700: 4.433603\n",
            "loss after iteration 33800: 4.424726\n",
            "loss after iteration 33900: 4.415848\n",
            "loss after iteration 34000: 4.406971\n",
            "loss after iteration 34100: 4.398096\n",
            "loss after iteration 34200: 4.389223\n",
            "loss after iteration 34300: 4.380352\n",
            "loss after iteration 34400: 4.371486\n",
            "loss after iteration 34500: 4.362624\n",
            "loss after iteration 34600: 4.353767\n",
            "loss after iteration 34700: 4.344916\n",
            "loss after iteration 34800: 4.336071\n",
            "loss after iteration 34900: 4.327234\n",
            "loss after iteration 35000: 4.318405\n",
            "loss after iteration 35100: 4.309584\n",
            "loss after iteration 35200: 4.300773\n",
            "loss after iteration 35300: 4.291972\n",
            "loss after iteration 35400: 4.283182\n",
            "loss after iteration 35500: 4.274403\n",
            "loss after iteration 35600: 4.265635\n",
            "loss after iteration 35700: 4.256880\n",
            "loss after iteration 35800: 4.248139\n",
            "loss after iteration 35900: 4.239410\n",
            "loss after iteration 36000: 4.230696\n",
            "loss after iteration 36100: 4.221996\n",
            "loss after iteration 36200: 4.213311\n",
            "loss after iteration 36300: 4.204641\n",
            "loss after iteration 36400: 4.195987\n",
            "loss after iteration 36500: 4.187350\n",
            "loss after iteration 36600: 4.178728\n",
            "loss after iteration 36700: 4.170124\n",
            "loss after iteration 36800: 4.161536\n",
            "loss after iteration 36900: 4.152966\n",
            "loss after iteration 37000: 4.144414\n",
            "loss after iteration 37100: 4.135879\n",
            "loss after iteration 37200: 4.127363\n",
            "loss after iteration 37300: 4.118864\n",
            "loss after iteration 37400: 4.110384\n",
            "loss after iteration 37500: 4.101923\n",
            "loss after iteration 37600: 4.093480\n",
            "loss after iteration 37700: 4.085057\n",
            "loss after iteration 37800: 4.076652\n",
            "loss after iteration 37900: 4.068266\n",
            "loss after iteration 38000: 4.059899\n",
            "loss after iteration 38100: 4.051552\n",
            "loss after iteration 38200: 4.043224\n",
            "loss after iteration 38300: 4.034915\n",
            "loss after iteration 38400: 4.026626\n",
            "loss after iteration 38500: 4.018356\n",
            "loss after iteration 38600: 4.010106\n",
            "loss after iteration 38700: 4.001875\n",
            "loss after iteration 38800: 3.993664\n",
            "loss after iteration 38900: 3.985472\n",
            "loss after iteration 39000: 3.977301\n",
            "loss after iteration 39100: 3.969149\n",
            "loss after iteration 39200: 3.961017\n",
            "loss after iteration 39300: 3.952905\n",
            "loss after iteration 39400: 3.944813\n",
            "loss after iteration 39500: 3.936741\n",
            "loss after iteration 39600: 3.928689\n",
            "loss after iteration 39700: 3.920657\n",
            "loss after iteration 39800: 3.912646\n",
            "loss after iteration 39900: 3.904655\n",
            "loss after iteration 40000: 3.896684\n",
            "loss after iteration 40100: 3.888734\n",
            "loss after iteration 40200: 3.880805\n",
            "loss after iteration 40300: 3.872897\n",
            "loss after iteration 40400: 3.865010\n",
            "loss after iteration 40500: 3.857144\n",
            "loss after iteration 40600: 3.849299\n",
            "loss after iteration 40700: 3.841475\n",
            "loss after iteration 40800: 3.833673\n",
            "loss after iteration 40900: 3.825893\n",
            "loss after iteration 41000: 3.818135\n",
            "loss after iteration 41100: 3.810398\n",
            "loss after iteration 41200: 3.802684\n",
            "loss after iteration 41300: 3.794992\n",
            "loss after iteration 41400: 3.787322\n",
            "loss after iteration 41500: 3.779675\n",
            "loss after iteration 41600: 3.772051\n",
            "loss after iteration 41700: 3.764449\n",
            "loss after iteration 41800: 3.756870\n",
            "loss after iteration 41900: 3.749315\n",
            "loss after iteration 42000: 3.741783\n",
            "loss after iteration 42100: 3.734274\n",
            "loss after iteration 42200: 3.726788\n",
            "loss after iteration 42300: 3.719326\n",
            "loss after iteration 42400: 3.711888\n",
            "loss after iteration 42500: 3.704474\n",
            "loss after iteration 42600: 3.697083\n",
            "loss after iteration 42700: 3.689716\n",
            "loss after iteration 42800: 3.682373\n",
            "loss after iteration 42900: 3.675054\n",
            "loss after iteration 43000: 3.667760\n",
            "loss after iteration 43100: 3.660489\n",
            "loss after iteration 43200: 3.653242\n",
            "loss after iteration 43300: 3.646020\n",
            "loss after iteration 43400: 3.638821\n",
            "loss after iteration 43500: 3.631647\n",
            "loss after iteration 43600: 3.624496\n",
            "loss after iteration 43700: 3.617370\n",
            "loss after iteration 43800: 3.610267\n",
            "loss after iteration 43900: 3.603189\n",
            "loss after iteration 44000: 3.596134\n",
            "loss after iteration 44100: 3.589103\n",
            "loss after iteration 44200: 3.582095\n",
            "loss after iteration 44300: 3.575111\n",
            "loss after iteration 44400: 3.568150\n",
            "loss after iteration 44500: 3.561212\n",
            "loss after iteration 44600: 3.554298\n",
            "loss after iteration 44700: 3.547406\n",
            "loss after iteration 44800: 3.540537\n",
            "loss after iteration 44900: 3.533690\n",
            "loss after iteration 45000: 3.526865\n",
            "loss after iteration 45100: 3.520063\n",
            "loss after iteration 45200: 3.513282\n",
            "loss after iteration 45300: 3.506523\n",
            "loss after iteration 45400: 3.499785\n",
            "loss after iteration 45500: 3.493069\n",
            "loss after iteration 45600: 3.486373\n",
            "loss after iteration 45700: 3.479697\n",
            "loss after iteration 45800: 3.473042\n",
            "loss after iteration 45900: 3.466406\n",
            "loss after iteration 46000: 3.459791\n",
            "loss after iteration 46100: 3.453194\n",
            "loss after iteration 46200: 3.446616\n",
            "loss after iteration 46300: 3.440057\n",
            "loss after iteration 46400: 3.433517\n",
            "loss after iteration 46500: 3.426994\n",
            "loss after iteration 46600: 3.420489\n",
            "loss after iteration 46700: 3.414001\n",
            "loss after iteration 46800: 3.407529\n",
            "loss after iteration 46900: 3.401075\n",
            "loss after iteration 47000: 3.394636\n",
            "loss after iteration 47100: 3.388213\n",
            "loss after iteration 47200: 3.381806\n",
            "loss after iteration 47300: 3.375414\n",
            "loss after iteration 47400: 3.369036\n",
            "loss after iteration 47500: 3.362673\n",
            "loss after iteration 47600: 3.356323\n",
            "loss after iteration 47700: 3.349987\n",
            "loss after iteration 47800: 3.343664\n",
            "loss after iteration 47900: 3.337354\n",
            "loss after iteration 48000: 3.331057\n",
            "loss after iteration 48100: 3.324771\n",
            "loss after iteration 48200: 3.318497\n",
            "loss after iteration 48300: 3.312235\n",
            "loss after iteration 48400: 3.305984\n",
            "loss after iteration 48500: 3.299743\n",
            "loss after iteration 48600: 3.293512\n",
            "loss after iteration 48700: 3.287292\n",
            "loss after iteration 48800: 3.281081\n",
            "loss after iteration 48900: 3.274880\n",
            "loss after iteration 49000: 3.268688\n",
            "loss after iteration 49100: 3.262504\n",
            "loss after iteration 49200: 3.256329\n",
            "loss after iteration 49300: 3.250163\n",
            "loss after iteration 49400: 3.244004\n",
            "loss after iteration 49500: 3.237853\n",
            "loss after iteration 49600: 3.231709\n",
            "loss after iteration 49700: 3.225573\n",
            "loss after iteration 49800: 3.219443\n",
            "loss after iteration 49900: 3.213320\n",
            "loss after iteration 50000: 3.207204\n",
            "loss after iteration 50100: 3.201094\n",
            "loss after iteration 50200: 3.194990\n",
            "loss after iteration 50300: 3.188892\n",
            "loss after iteration 50400: 3.182800\n",
            "loss after iteration 50500: 3.176713\n",
            "loss after iteration 50600: 3.170632\n",
            "loss after iteration 50700: 3.164556\n",
            "loss after iteration 50800: 3.158486\n",
            "loss after iteration 50900: 3.152420\n",
            "loss after iteration 51000: 3.146360\n",
            "loss after iteration 51100: 3.140304\n",
            "loss after iteration 51200: 3.134253\n",
            "loss after iteration 51300: 3.128206\n",
            "loss after iteration 51400: 3.122164\n",
            "loss after iteration 51500: 3.116127\n",
            "loss after iteration 51600: 3.110094\n",
            "loss after iteration 51700: 3.104066\n",
            "loss after iteration 51800: 3.098042\n",
            "loss after iteration 51900: 3.092022\n",
            "loss after iteration 52000: 3.086006\n",
            "loss after iteration 52100: 3.079995\n",
            "loss after iteration 52200: 3.073988\n",
            "loss after iteration 52300: 3.067985\n",
            "loss after iteration 52400: 3.061986\n",
            "loss after iteration 52500: 3.055992\n",
            "loss after iteration 52600: 3.050001\n",
            "loss after iteration 52700: 3.044015\n",
            "loss after iteration 52800: 3.038034\n",
            "loss after iteration 52900: 3.032056\n",
            "loss after iteration 53000: 3.026083\n",
            "loss after iteration 53100: 3.020114\n",
            "loss after iteration 53200: 3.014149\n",
            "loss after iteration 53300: 3.008189\n",
            "loss after iteration 53400: 3.002233\n",
            "loss after iteration 53500: 2.996282\n",
            "loss after iteration 53600: 2.990335\n",
            "loss after iteration 53700: 2.984393\n",
            "loss after iteration 53800: 2.978455\n",
            "loss after iteration 53900: 2.972522\n",
            "loss after iteration 54000: 2.966594\n",
            "loss after iteration 54100: 2.960671\n",
            "loss after iteration 54200: 2.954752\n",
            "loss after iteration 54300: 2.948839\n",
            "loss after iteration 54400: 2.942930\n",
            "loss after iteration 54500: 2.937027\n",
            "loss after iteration 54600: 2.931129\n",
            "loss after iteration 54700: 2.925236\n",
            "loss after iteration 54800: 2.919348\n",
            "loss after iteration 54900: 2.913466\n",
            "loss after iteration 55000: 2.907589\n",
            "loss after iteration 55100: 2.901718\n",
            "loss after iteration 55200: 2.895852\n",
            "loss after iteration 55300: 2.889992\n",
            "loss after iteration 55400: 2.884138\n",
            "loss after iteration 55500: 2.878289\n",
            "loss after iteration 55600: 2.872447\n",
            "loss after iteration 55700: 2.866611\n",
            "loss after iteration 55800: 2.860781\n",
            "loss after iteration 55900: 2.854957\n",
            "loss after iteration 56000: 2.849139\n",
            "loss after iteration 56100: 2.843328\n",
            "loss after iteration 56200: 2.837524\n",
            "loss after iteration 56300: 2.831725\n",
            "loss after iteration 56400: 2.825934\n",
            "loss after iteration 56500: 2.820149\n",
            "loss after iteration 56600: 2.814372\n",
            "loss after iteration 56700: 2.808601\n",
            "loss after iteration 56800: 2.802837\n",
            "loss after iteration 56900: 2.797080\n",
            "loss after iteration 57000: 2.791331\n",
            "loss after iteration 57100: 2.785589\n",
            "loss after iteration 57200: 2.779854\n",
            "loss after iteration 57300: 2.774127\n",
            "loss after iteration 57400: 2.768407\n",
            "loss after iteration 57500: 2.762695\n",
            "loss after iteration 57600: 2.756991\n",
            "loss after iteration 57700: 2.751295\n",
            "loss after iteration 57800: 2.745606\n",
            "loss after iteration 57900: 2.739926\n",
            "loss after iteration 58000: 2.734254\n",
            "loss after iteration 58100: 2.728590\n",
            "loss after iteration 58200: 2.722934\n",
            "loss after iteration 58300: 2.717287\n",
            "loss after iteration 58400: 2.711648\n",
            "loss after iteration 58500: 2.706017\n",
            "loss after iteration 58600: 2.700396\n",
            "loss after iteration 58700: 2.694783\n",
            "loss after iteration 58800: 2.689179\n",
            "loss after iteration 58900: 2.683584\n",
            "loss after iteration 59000: 2.677998\n",
            "loss after iteration 59100: 2.672421\n",
            "loss after iteration 59200: 2.666853\n",
            "loss after iteration 59300: 2.661294\n",
            "loss after iteration 59400: 2.655745\n",
            "loss after iteration 59500: 2.650205\n",
            "loss after iteration 59600: 2.644675\n",
            "loss after iteration 59700: 2.639154\n",
            "loss after iteration 59800: 2.633643\n",
            "loss after iteration 59900: 2.628141\n",
            "loss after iteration 60000: 2.622650\n",
            "loss after iteration 60100: 2.617168\n",
            "loss after iteration 60200: 2.611696\n",
            "loss after iteration 60300: 2.606235\n",
            "loss after iteration 60400: 2.600783\n",
            "loss after iteration 60500: 2.595342\n",
            "loss after iteration 60600: 2.589911\n",
            "loss after iteration 60700: 2.584490\n",
            "loss after iteration 60800: 2.579079\n",
            "loss after iteration 60900: 2.573680\n",
            "loss after iteration 61000: 2.568290\n",
            "loss after iteration 61100: 2.562912\n",
            "loss after iteration 61200: 2.557543\n",
            "loss after iteration 61300: 2.552186\n",
            "loss after iteration 61400: 2.546840\n",
            "loss after iteration 61500: 2.541504\n",
            "loss after iteration 61600: 2.536179\n",
            "loss after iteration 61700: 2.530866\n",
            "loss after iteration 61800: 2.525563\n",
            "loss after iteration 61900: 2.520271\n",
            "loss after iteration 62000: 2.514991\n",
            "loss after iteration 62100: 2.509722\n",
            "loss after iteration 62200: 2.504464\n",
            "loss after iteration 62300: 2.499217\n",
            "loss after iteration 62400: 2.493982\n",
            "loss after iteration 62500: 2.488758\n",
            "loss after iteration 62600: 2.483546\n",
            "loss after iteration 62700: 2.478345\n",
            "loss after iteration 62800: 2.473156\n",
            "loss after iteration 62900: 2.467979\n",
            "loss after iteration 63000: 2.462813\n",
            "loss after iteration 63100: 2.457659\n",
            "loss after iteration 63200: 2.452516\n",
            "loss after iteration 63300: 2.447386\n",
            "loss after iteration 63400: 2.442267\n",
            "loss after iteration 63500: 2.437160\n",
            "loss after iteration 63600: 2.432065\n",
            "loss after iteration 63700: 2.426982\n",
            "loss after iteration 63800: 2.421911\n",
            "loss after iteration 63900: 2.416852\n",
            "loss after iteration 64000: 2.411805\n",
            "loss after iteration 64100: 2.406770\n",
            "loss after iteration 64200: 2.401747\n",
            "loss after iteration 64300: 2.396737\n",
            "loss after iteration 64400: 2.391738\n",
            "loss after iteration 64500: 2.386752\n",
            "loss after iteration 64600: 2.381778\n",
            "loss after iteration 64700: 2.376816\n",
            "loss after iteration 64800: 2.371867\n",
            "loss after iteration 64900: 2.366929\n",
            "loss after iteration 65000: 2.362004\n",
            "loss after iteration 65100: 2.357092\n",
            "loss after iteration 65200: 2.352191\n",
            "loss after iteration 65300: 2.347304\n",
            "loss after iteration 65400: 2.342428\n",
            "loss after iteration 65500: 2.337565\n",
            "loss after iteration 65600: 2.332714\n",
            "loss after iteration 65700: 2.327876\n",
            "loss after iteration 65800: 2.323050\n",
            "loss after iteration 65900: 2.318237\n",
            "loss after iteration 66000: 2.313436\n",
            "loss after iteration 66100: 2.308647\n",
            "loss after iteration 66200: 2.303871\n",
            "loss after iteration 66300: 2.299108\n",
            "loss after iteration 66400: 2.294357\n",
            "loss after iteration 66500: 2.289618\n",
            "loss after iteration 66600: 2.284892\n",
            "loss after iteration 66700: 2.280178\n",
            "loss after iteration 66800: 2.275477\n",
            "loss after iteration 66900: 2.270789\n",
            "loss after iteration 67000: 2.266112\n",
            "loss after iteration 67100: 2.261449\n",
            "loss after iteration 67200: 2.256798\n",
            "loss after iteration 67300: 2.252159\n",
            "loss after iteration 67400: 2.247533\n",
            "loss after iteration 67500: 2.242919\n",
            "loss after iteration 67600: 2.238318\n",
            "loss after iteration 67700: 2.233729\n",
            "loss after iteration 67800: 2.229153\n",
            "loss after iteration 67900: 2.224589\n",
            "loss after iteration 68000: 2.220038\n",
            "loss after iteration 68100: 2.215499\n",
            "loss after iteration 68200: 2.210973\n",
            "loss after iteration 68300: 2.206459\n",
            "loss after iteration 68400: 2.201957\n",
            "loss after iteration 68500: 2.197468\n",
            "loss after iteration 68600: 2.192991\n",
            "loss after iteration 68700: 2.188527\n",
            "loss after iteration 68800: 2.184075\n",
            "loss after iteration 68900: 2.179635\n",
            "loss after iteration 69000: 2.175207\n",
            "loss after iteration 69100: 2.170792\n",
            "loss after iteration 69200: 2.166390\n",
            "loss after iteration 69300: 2.161999\n",
            "loss after iteration 69400: 2.157621\n",
            "loss after iteration 69500: 2.153255\n",
            "loss after iteration 69600: 2.148901\n",
            "loss after iteration 69700: 2.144560\n",
            "loss after iteration 69800: 2.140230\n",
            "loss after iteration 69900: 2.135913\n",
            "loss after iteration 70000: 2.131608\n",
            "loss after iteration 70100: 2.127315\n",
            "loss after iteration 70200: 2.123034\n",
            "loss after iteration 70300: 2.118766\n",
            "loss after iteration 70400: 2.114509\n",
            "loss after iteration 70500: 2.110264\n",
            "loss after iteration 70600: 2.106032\n",
            "loss after iteration 70700: 2.101811\n",
            "loss after iteration 70800: 2.097603\n",
            "loss after iteration 70900: 2.093406\n",
            "loss after iteration 71000: 2.089221\n",
            "loss after iteration 71100: 2.085049\n",
            "loss after iteration 71200: 2.080888\n",
            "loss after iteration 71300: 2.076738\n",
            "loss after iteration 71400: 2.072601\n",
            "loss after iteration 71500: 2.068476\n",
            "loss after iteration 71600: 2.064362\n",
            "loss after iteration 71700: 2.060260\n",
            "loss after iteration 71800: 2.056170\n",
            "loss after iteration 71900: 2.052091\n",
            "loss after iteration 72000: 2.048024\n",
            "loss after iteration 72100: 2.043969\n",
            "loss after iteration 72200: 2.039926\n",
            "loss after iteration 72300: 2.035893\n",
            "loss after iteration 72400: 2.031873\n",
            "loss after iteration 72500: 2.027864\n",
            "loss after iteration 72600: 2.023866\n",
            "loss after iteration 72700: 2.019880\n",
            "loss after iteration 72800: 2.015906\n",
            "loss after iteration 72900: 2.011943\n",
            "loss after iteration 73000: 2.007991\n",
            "loss after iteration 73100: 2.004050\n",
            "loss after iteration 73200: 2.000121\n",
            "loss after iteration 73300: 1.996203\n",
            "loss after iteration 73400: 1.992297\n",
            "loss after iteration 73500: 1.988401\n",
            "loss after iteration 73600: 1.984517\n",
            "loss after iteration 73700: 1.980644\n",
            "loss after iteration 73800: 1.976782\n",
            "loss after iteration 73900: 1.972931\n",
            "loss after iteration 74000: 1.969091\n",
            "loss after iteration 74100: 1.965262\n",
            "loss after iteration 74200: 1.961444\n",
            "loss after iteration 74300: 1.957638\n",
            "loss after iteration 74400: 1.953842\n",
            "loss after iteration 74500: 1.950057\n",
            "loss after iteration 74600: 1.946283\n",
            "loss after iteration 74700: 1.942519\n",
            "loss after iteration 74800: 1.938767\n",
            "loss after iteration 74900: 1.935025\n",
            "loss after iteration 75000: 1.931294\n",
            "loss after iteration 75100: 1.927574\n",
            "loss after iteration 75200: 1.923864\n",
            "loss after iteration 75300: 1.920165\n",
            "loss after iteration 75400: 1.916477\n",
            "loss after iteration 75500: 1.912799\n",
            "loss after iteration 75600: 1.909132\n",
            "loss after iteration 75700: 1.905475\n",
            "loss after iteration 75800: 1.901829\n",
            "loss after iteration 75900: 1.898193\n",
            "loss after iteration 76000: 1.894568\n",
            "loss after iteration 76100: 1.890953\n",
            "loss after iteration 76200: 1.887348\n",
            "loss after iteration 76300: 1.883754\n",
            "loss after iteration 76400: 1.880170\n",
            "loss after iteration 76500: 1.876596\n",
            "loss after iteration 76600: 1.873032\n",
            "loss after iteration 76700: 1.869479\n",
            "loss after iteration 76800: 1.865935\n",
            "loss after iteration 76900: 1.862402\n",
            "loss after iteration 77000: 1.858879\n",
            "loss after iteration 77100: 1.855366\n",
            "loss after iteration 77200: 1.851863\n",
            "loss after iteration 77300: 1.848370\n",
            "loss after iteration 77400: 1.844886\n",
            "loss after iteration 77500: 1.841413\n",
            "loss after iteration 77600: 1.837950\n",
            "loss after iteration 77700: 1.834496\n",
            "loss after iteration 77800: 1.831052\n",
            "loss after iteration 77900: 1.827618\n",
            "loss after iteration 78000: 1.824194\n",
            "loss after iteration 78100: 1.820780\n",
            "loss after iteration 78200: 1.817375\n",
            "loss after iteration 78300: 1.813979\n",
            "loss after iteration 78400: 1.810594\n",
            "loss after iteration 78500: 1.807218\n",
            "loss after iteration 78600: 1.803851\n",
            "loss after iteration 78700: 1.800494\n",
            "loss after iteration 78800: 1.797147\n",
            "loss after iteration 78900: 1.793809\n",
            "loss after iteration 79000: 1.790480\n",
            "loss after iteration 79100: 1.787161\n",
            "loss after iteration 79200: 1.783851\n",
            "loss after iteration 79300: 1.780550\n",
            "loss after iteration 79400: 1.777259\n",
            "loss after iteration 79500: 1.773976\n",
            "loss after iteration 79600: 1.770704\n",
            "loss after iteration 79700: 1.767440\n",
            "loss after iteration 79800: 1.764185\n",
            "loss after iteration 79900: 1.760940\n",
            "loss after iteration 80000: 1.757703\n",
            "loss after iteration 80100: 1.754476\n",
            "loss after iteration 80200: 1.751258\n",
            "loss after iteration 80300: 1.748048\n",
            "loss after iteration 80400: 1.744848\n",
            "loss after iteration 80500: 1.741656\n",
            "loss after iteration 80600: 1.738474\n",
            "loss after iteration 80700: 1.735300\n",
            "loss after iteration 80800: 1.732135\n",
            "loss after iteration 80900: 1.728979\n",
            "loss after iteration 81000: 1.725831\n",
            "loss after iteration 81100: 1.722693\n",
            "loss after iteration 81200: 1.719563\n",
            "loss after iteration 81300: 1.716442\n",
            "loss after iteration 81400: 1.713329\n",
            "loss after iteration 81500: 1.710225\n",
            "loss after iteration 81600: 1.707129\n",
            "loss after iteration 81700: 1.704042\n",
            "loss after iteration 81800: 1.700964\n",
            "loss after iteration 81900: 1.697894\n",
            "loss after iteration 82000: 1.694832\n",
            "loss after iteration 82100: 1.691779\n",
            "loss after iteration 82200: 1.688734\n",
            "loss after iteration 82300: 1.685698\n",
            "loss after iteration 82400: 1.682670\n",
            "loss after iteration 82500: 1.679650\n",
            "loss after iteration 82600: 1.676639\n",
            "loss after iteration 82700: 1.673635\n",
            "loss after iteration 82800: 1.670640\n",
            "loss after iteration 82900: 1.667653\n",
            "loss after iteration 83000: 1.664674\n",
            "loss after iteration 83100: 1.661704\n",
            "loss after iteration 83200: 1.658741\n",
            "loss after iteration 83300: 1.655787\n",
            "loss after iteration 83400: 1.652840\n",
            "loss after iteration 83500: 1.649901\n",
            "loss after iteration 83600: 1.646971\n",
            "loss after iteration 83700: 1.644048\n",
            "loss after iteration 83800: 1.641133\n",
            "loss after iteration 83900: 1.638226\n",
            "loss after iteration 84000: 1.635327\n",
            "loss after iteration 84100: 1.632436\n",
            "loss after iteration 84200: 1.629553\n",
            "loss after iteration 84300: 1.626677\n",
            "loss after iteration 84400: 1.623809\n",
            "loss after iteration 84500: 1.620949\n",
            "loss after iteration 84600: 1.618096\n",
            "loss after iteration 84700: 1.615251\n",
            "loss after iteration 84800: 1.612413\n",
            "loss after iteration 84900: 1.609584\n",
            "loss after iteration 85000: 1.606761\n",
            "loss after iteration 85100: 1.603947\n",
            "loss after iteration 85200: 1.601139\n",
            "loss after iteration 85300: 1.598340\n",
            "loss after iteration 85400: 1.595547\n",
            "loss after iteration 85500: 1.592762\n",
            "loss after iteration 85600: 1.589985\n",
            "loss after iteration 85700: 1.587214\n",
            "loss after iteration 85800: 1.584452\n",
            "loss after iteration 85900: 1.581696\n",
            "loss after iteration 86000: 1.578948\n",
            "loss after iteration 86100: 1.576207\n",
            "loss after iteration 86200: 1.573473\n",
            "loss after iteration 86300: 1.570746\n",
            "loss after iteration 86400: 1.568027\n",
            "loss after iteration 86500: 1.565314\n",
            "loss after iteration 86600: 1.562609\n",
            "loss after iteration 86700: 1.559911\n",
            "loss after iteration 86800: 1.557220\n",
            "loss after iteration 86900: 1.554536\n",
            "loss after iteration 87000: 1.551859\n",
            "loss after iteration 87100: 1.549188\n",
            "loss after iteration 87200: 1.546525\n",
            "loss after iteration 87300: 1.543869\n",
            "loss after iteration 87400: 1.541220\n",
            "loss after iteration 87500: 1.538577\n",
            "loss after iteration 87600: 1.535942\n",
            "loss after iteration 87700: 1.533313\n",
            "loss after iteration 87800: 1.530691\n",
            "loss after iteration 87900: 1.528076\n",
            "loss after iteration 88000: 1.525467\n",
            "loss after iteration 88100: 1.522865\n",
            "loss after iteration 88200: 1.520270\n",
            "loss after iteration 88300: 1.517682\n",
            "loss after iteration 88400: 1.515100\n",
            "loss after iteration 88500: 1.512525\n",
            "loss after iteration 88600: 1.509957\n",
            "loss after iteration 88700: 1.507395\n",
            "loss after iteration 88800: 1.504839\n",
            "loss after iteration 88900: 1.502290\n",
            "loss after iteration 89000: 1.499748\n",
            "loss after iteration 89100: 1.497212\n",
            "loss after iteration 89200: 1.494682\n",
            "loss after iteration 89300: 1.492159\n",
            "loss after iteration 89400: 1.489643\n",
            "loss after iteration 89500: 1.487132\n",
            "loss after iteration 89600: 1.484628\n",
            "loss after iteration 89700: 1.482131\n",
            "loss after iteration 89800: 1.479639\n",
            "loss after iteration 89900: 1.477154\n",
            "loss after iteration 90000: 1.474676\n",
            "loss after iteration 90100: 1.472203\n",
            "loss after iteration 90200: 1.469737\n",
            "loss after iteration 90300: 1.467277\n",
            "loss after iteration 90400: 1.464823\n",
            "loss after iteration 90500: 1.462375\n",
            "loss after iteration 90600: 1.459933\n",
            "loss after iteration 90700: 1.457498\n",
            "loss after iteration 90800: 1.455068\n",
            "loss after iteration 90900: 1.452645\n",
            "loss after iteration 91000: 1.450227\n",
            "loss after iteration 91100: 1.447816\n",
            "loss after iteration 91200: 1.445410\n",
            "loss after iteration 91300: 1.443011\n",
            "loss after iteration 91400: 1.440617\n",
            "loss after iteration 91500: 1.438229\n",
            "loss after iteration 91600: 1.435848\n",
            "loss after iteration 91700: 1.433472\n",
            "loss after iteration 91800: 1.431102\n",
            "loss after iteration 91900: 1.428738\n",
            "loss after iteration 92000: 1.426379\n",
            "loss after iteration 92100: 1.424027\n",
            "loss after iteration 92200: 1.421680\n",
            "loss after iteration 92300: 1.419339\n",
            "loss after iteration 92400: 1.417003\n",
            "loss after iteration 92500: 1.414674\n",
            "loss after iteration 92600: 1.412350\n",
            "loss after iteration 92700: 1.410031\n",
            "loss after iteration 92800: 1.407719\n",
            "loss after iteration 92900: 1.405412\n",
            "loss after iteration 93000: 1.403110\n",
            "loss after iteration 93100: 1.400815\n",
            "loss after iteration 93200: 1.398524\n",
            "loss after iteration 93300: 1.396239\n",
            "loss after iteration 93400: 1.393960\n",
            "loss after iteration 93500: 1.391687\n",
            "loss after iteration 93600: 1.389418\n",
            "loss after iteration 93700: 1.387156\n",
            "loss after iteration 93800: 1.384898\n",
            "loss after iteration 93900: 1.382646\n",
            "loss after iteration 94000: 1.380400\n",
            "loss after iteration 94100: 1.378159\n",
            "loss after iteration 94200: 1.375923\n",
            "loss after iteration 94300: 1.373693\n",
            "loss after iteration 94400: 1.371467\n",
            "loss after iteration 94500: 1.369248\n",
            "loss after iteration 94600: 1.367033\n",
            "loss after iteration 94700: 1.364824\n",
            "loss after iteration 94800: 1.362620\n",
            "loss after iteration 94900: 1.360421\n",
            "loss after iteration 95000: 1.358228\n",
            "loss after iteration 95100: 1.356040\n",
            "loss after iteration 95200: 1.353856\n",
            "loss after iteration 95300: 1.351678\n",
            "loss after iteration 95400: 1.349506\n",
            "loss after iteration 95500: 1.347338\n",
            "loss after iteration 95600: 1.345175\n",
            "loss after iteration 95700: 1.343018\n",
            "loss after iteration 95800: 1.340865\n",
            "loss after iteration 95900: 1.338718\n",
            "loss after iteration 96000: 1.336575\n",
            "loss after iteration 96100: 1.334438\n",
            "loss after iteration 96200: 1.332306\n",
            "loss after iteration 96300: 1.330178\n",
            "loss after iteration 96400: 1.328056\n",
            "loss after iteration 96500: 1.325939\n",
            "loss after iteration 96600: 1.323826\n",
            "loss after iteration 96700: 1.321718\n",
            "loss after iteration 96800: 1.319616\n",
            "loss after iteration 96900: 1.317518\n",
            "loss after iteration 97000: 1.315425\n",
            "loss after iteration 97100: 1.313337\n",
            "loss after iteration 97200: 1.311253\n",
            "loss after iteration 97300: 1.309175\n",
            "loss after iteration 97400: 1.307101\n",
            "loss after iteration 97500: 1.305032\n",
            "loss after iteration 97600: 1.302968\n",
            "loss after iteration 97700: 1.300909\n",
            "loss after iteration 97800: 1.298854\n",
            "loss after iteration 97900: 1.296804\n",
            "loss after iteration 98000: 1.294759\n",
            "loss after iteration 98100: 1.292718\n",
            "loss after iteration 98200: 1.290683\n",
            "loss after iteration 98300: 1.288651\n",
            "loss after iteration 98400: 1.286625\n",
            "loss after iteration 98500: 1.284603\n",
            "loss after iteration 98600: 1.282585\n",
            "loss after iteration 98700: 1.280573\n",
            "loss after iteration 98800: 1.278564\n",
            "loss after iteration 98900: 1.276561\n",
            "loss after iteration 99000: 1.274562\n",
            "loss after iteration 99100: 1.272567\n",
            "loss after iteration 99200: 1.270577\n",
            "loss after iteration 99300: 1.268591\n",
            "loss after iteration 99400: 1.266610\n",
            "loss after iteration 99500: 1.264634\n",
            "loss after iteration 99600: 1.262662\n",
            "loss after iteration 99700: 1.260694\n",
            "loss after iteration 99800: 1.258731\n",
            "loss after iteration 99900: 1.256772\n"
          ]
        }
      ],
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters = NeuralNetwork(trainx, trainy, 5, num_iterations = 100000, learning_rate = 0.01, print_loss=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8vFWiwz3FWA"
      },
      "source": [
        "## Check the accuracy on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "BUdvhN9BoXDn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, trainx)\n",
        "print(accuracy_score(Y.T, predictions.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjg3DINw3FWB"
      },
      "source": [
        "## Check the accuracy on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "2-pHP3ts3FWB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "predictions_test = predict(parameters,testx)\n",
        "print(accuracy_score(testy.T, predictions_test.T))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29d85114910>"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "confusion = confusion_matrix(testy.T, predictions_test.T)\n",
        "ConfusionMatrixDisplay(confusion).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrT2xrXeoXDz"
      },
      "source": [
        "## Run the model multiple times with different hyperparameters and write your interpretation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preform a simple grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.01, 'tanh', 1)\n"
          ]
        }
      ],
      "source": [
        "params = {'learning_rate': (0.01, 0.05, 0.1),\n",
        "        'a1': (np.tanh, lambda x: np.where(x>0, 1 , 0), sigmoid),\n",
        "        'n_h' : range(1,15,2)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for lr in params['learning_rate']:\n",
        "    for a1 in params['a1']:\n",
        "        for n_h in params['n_h']:\n",
        "                parameters = NeuralNetwork(trainx, trainy, n_h= n_h, num_iterations = 1000, learning_rate = lr, print_loss=False, a1=a1, a2=sigmoid, loss_function = cross_entropy_loss)\n",
        "                predictions_test = predict(parameters, testx)\n",
        "                results[(lr, a1.__name__, n_h)] = accuracy_score(testy.T, predictions_test.T)\n",
        "\n",
        "print(max(results,  key= lambda x: results[x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66OLHmwvoXD0"
      },
      "source": [
        "##### Q:What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72jTqgYDe1vI"
      },
      "source": [
        "### `The results of the grid search implies that adding more neurons in the hidden layer raises the possibility of overfitting.`\n",
        "### `hence, the optimal hyperparmeters selection to this problems is:`\n",
        "* `learning_rate` = 0.01\n",
        "* `activation_function1` = tanh\n",
        "* `num_of_neurons in the hidden layer` = 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Neural Network From Scratch.ipynb",
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "wRuwL",
      "launcher_item_id": "NI888"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "7037ec29b621de18a3396a0455ec45ad55ce896f0aa235683271d94d3867e1de"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
