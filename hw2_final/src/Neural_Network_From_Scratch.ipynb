{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A4Q6DBB3FVY"
      },
      "source": [
        "# Neural Network Implementation-From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ7Bd9ZsoXCR"
      },
      "source": [
        "## A. Neural Network: Binary Classification \n",
        "\n",
        "In this assignment, we will learn to build a fully-connected neural network with standard architecture, i.e., only one hidden layer. We will use the `Breast cancer wisconsin (diagnostic) dataset` available in `https://scikit-learn.org/stable/datasets/index.html`. It has a total of 569 sample with two classes (Malignant and Benign). Each sample has 30 real-valued features. \n",
        "\n",
        "**This assignment will help you understand and implement:**\n",
        "- A neural network for binary classification consisting of one hidden layer with non-linear activation. You will implement ideas like forward propogation, computing the loss, bagward propogation, and parameter(weights) update. Using the trained network, you will learn to predict a class given the features of a sample.\n",
        "\n",
        "**Note:** There are multiple conventions for coding neural networks. We will follow the conventions suggested by Andrew Ng: https://www.coursera.org/learn/neural-networks-deep-learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sqaGHZSJoXCU"
      },
      "outputs": [],
      "source": [
        "# Package imports\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.linear_model\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGkbrWCS3FVm"
      },
      "source": [
        "## 1. Loading the dataset and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LENi5Q9Z3FVn"
      },
      "outputs": [],
      "source": [
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size =0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LkzSuMSr3FVo"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rpzj7FmV3FVq"
      },
      "outputs": [],
      "source": [
        "trainx = train_data.T\n",
        "trainy = train_labels.reshape(-1,1).T\n",
        "\n",
        "testx = test_data.T\n",
        "testy =test_labels.reshape(-1,1).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d78k8Prk3FVs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((30, 455), (1, 455), (30, 114), (1, 114))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainx.shape, trainy.shape, testx.shape, testy.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0dIZ3-k23FVu"
      },
      "outputs": [],
      "source": [
        "X=trainx\n",
        "Y=trainy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DFAQt6HYoXCh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of training samples: 455\n",
            "Number of features per sample: 30\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "shape_X = X.shape\n",
        "shape_Y = Y.shape\n",
        "m = shape_X[1]  # training set size\n",
        "### END CODE HERE ###\n",
        "\n",
        "print ('No. of training samples: ' + str(m))\n",
        "print ('Number of features per sample: ' + str(shape_X[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2AFzm-hoXCv"
      },
      "source": [
        "## 2 - Neural Network model\n",
        "\n",
        "We will train a Neural Network with a single hidden layer.\n",
        "\n",
        "**Mathematically**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
        "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
        "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
        "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
        "$$y^{(i)}_{prediction} = \\begin{cases} 1 & {if } a^{[2](i)} > 0.5 \\\\ 0 & {otherwise } \\end{cases}\\tag{5}$$\n",
        "\n",
        "Given the predictions on all the examples, you can also compute the cost (loss) $J$ as follows: \n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
        "\n",
        "**Important**: Building the NN will involve the following:\n",
        "\n",
        "    1. Specify the network structure in terms of the number of input units, number of neurons in the hidden units, ...\n",
        "    2. Initialize the parameters of the model\n",
        "    3. Loop a number of iterations:\n",
        "        - Forward propagation\n",
        "        - Compute loss and the overall loss\n",
        "        - Backward propagation\n",
        "        - Update the parameters (gradient descent)\n",
        "\n",
        "In order to make the code modular, we can implement each of the step as a function and them combine them together to build the overall model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC3UrB-AoXCw"
      },
      "source": [
        "### 2.1 - Specify the network structure \n",
        "    - n_x: input layer size\n",
        "    - n_h: #neurons in  hidden layer (hard code a value, say 10) \n",
        "    - n_y: the size of the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mOsdX_bPoXCx"
      },
      "outputs": [],
      "source": [
        "def model_architecture(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### \n",
        "    n_x = X.shape[0] # size of input layer\n",
        "    n_h = 10\n",
        "    n_y = 1 # number ofsize of output layer\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    return (n_x, n_h, n_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4-FAtl-oXC3"
      },
      "source": [
        "### 2.2 - Initialize the parameters of the model\n",
        "\n",
        "- Initialize the weights matrices with random values. \n",
        "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
        "- Initialize the bias vectors as zeros. \n",
        "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XV3W6uLvoXC3"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(2)\n",
        "\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
        "    b1 = np.zeros((n_h,1))\n",
        "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
        "    b2 = np.zeros((n_y,1))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBRbnqYkoXC8"
      },
      "source": [
        "### 2.3 - The Loop ####\n",
        "\n",
        "**Instructions**:\n",
        "- Look above at the mathematical representation of your classifier.\n",
        "- Define the function `sigmoid()`.\n",
        "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
        "- The steps you have to implement are:\n",
        "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
        "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
        "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-4Xif_6C3FV3"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    ### Update THE CODE HERE ###\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "    ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AkJ4TKKSoXC9"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    ### START CODE HERE ### \n",
        "    Z1 = W1 @ X\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = W2 @ A1\n",
        "    A2 = sigmoid(Z2)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21VnrjU3oXDC"
      },
      "source": [
        "Now you have computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for every example, you can compute the loss function as follows:\n",
        "\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{7}$$\n",
        "\n",
        "- Implement the cross-entropy loss:\n",
        "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
        "```python\n",
        "logprobs = np.multiply(np.log(A2),Y)\n",
        "loss = - np.sum(logprobs)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "whxXuxq-oXDD"
      },
      "outputs": [],
      "source": [
        "def compute_loss(A2, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "       \n",
        "    Returns:\n",
        "    loss -- cross-entropy loss given equation (7)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1] # number of example\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    ### START CODE HERE ###\n",
        "    logprobs = np.multiply(np.log(A2),Y)\n",
        "    loss = -1 * np.sum(logprobs)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    loss = float(np.squeeze(loss))\n",
        "\n",
        "    assert(isinstance(loss, float))\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5puT2VVoXDH"
      },
      "source": [
        "Using the cache computed during forward propagation, we can now implement backward propagation.\n",
        "\n",
        "Backpropagation is usually the hardest (most mathematical) part. You can use the following six equations as vectorized implementation:\n",
        "\n",
        "$$dZ^{[2]} = A^{[2]} - Y \\tag{8}$$ \n",
        "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]}A^{[1]{T}} \\tag{9}$$ \n",
        "$$db^{[2]} = \\frac{1}{m} np.sum(dZ^{[2]}, axis = 1, keepdims = True)\\tag{10}$$ \n",
        "$$dZ^{[1]} = W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]}) \\tag{11}$$ \n",
        "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]}X^{{T}} \\tag{12}$$ \n",
        "$$db^{[1]} = \\frac{1}{m} np.sum(dZ^{[1]}, axis = 1, keepdims = True)\\tag{13}$$ \n",
        "\n",
        "- $*$ denotes elementwise multiplication.\n",
        "- Notations followed:\n",
        "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
        "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
        "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
        "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
        "   \n",
        "- Tips:\n",
        "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
        "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KHTVtf4_oXDJ"
      },
      "outputs": [],
      "source": [
        "def backprop(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data\n",
        "    Y -- \"true\" labels\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    ### START CODE HERE ### \n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    ### START CODE HERE ### \n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = ((1/m)* dZ2) @ A1.T\n",
        "    db2 = (1/m)* np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = ( W2.T @ dZ2 ) * (1 - np.power(A1,2))\n",
        "    dW1 = (1/m)*dZ1@ X.T\n",
        "    db1 = (1/m)* np.sum(dZ1, axis=1, keepdims=True)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsDhe51IoXDO"
      },
      "source": [
        "Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
        "\n",
        "**Gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xWeErPRmoXDP"
      },
      "outputs": [],
      "source": [
        "def update(parameters, grads, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    learning_rate -- The learning rate\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    ### START CODE HERE ### \n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "    ## END CODE HERE ###\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    ### START CODE HERE ### \n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSWq5OE4oXDT"
      },
      "source": [
        "### 2.4 - Integrate parts 2.1, 2.2 and 2.3 in NeuralNetwork() ####\n",
        "\n",
        "Build your neural network model in `NeuralNetwork()`.\n",
        "\n",
        "**Instructions**: The neural network model has to use the previous functions in the right order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cnSyk2auoXDU"
      },
      "outputs": [],
      "source": [
        "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_loss=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset\n",
        "    Y -- labels \n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    learning_rate -- The learning rate\n",
        "    print_loss -- if True, print the loss every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    n_x = model_architecture(X, Y)[0]\n",
        "    n_y = model_architecture(X, Y)[2]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### \n",
        "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        ### START CODE HERE ### \n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        \n",
        "        # loss function. Inputs: \"A2, Y, parameters\". Outputs: \"loss\".\n",
        "        loss = compute_loss(A2,Y)\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backprop(parameters,cache,X,Y)\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters =  update(parameters,grads)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Print the loss every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            print (\"loss after iteration %i: %f\" %(i, loss))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ud31U_ZoXDY"
      },
      "source": [
        "### 2.5 Predictions\n",
        "\n",
        "Use your model to predict by building predict().\n",
        "\n",
        "**Reminder**: $ predictions = \\begin{cases}\n",
        "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}$  \n",
        "    \n",
        "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YPMG_zMloXDZ"
      },
      "outputs": [],
      "source": [
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data \n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ### \n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = np.where(A2> 0.5, 1 , 0)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC3OHe-9oXDd"
      },
      "source": [
        "## 3. Model Execution\n",
        "It is time to run the model and see how it performs on the dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "op5NM0gXoXDi",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss after iteration 0: 200.899664\n",
            "loss after iteration 100: 198.426974\n",
            "loss after iteration 200: 177.315685\n",
            "loss after iteration 300: 126.463748\n",
            "loss after iteration 400: 85.705073\n",
            "loss after iteration 500: 62.315205\n",
            "loss after iteration 600: 48.803144\n",
            "loss after iteration 700: 40.386295\n",
            "loss after iteration 800: 34.720418\n",
            "loss after iteration 900: 30.666341\n",
            "loss after iteration 1000: 27.627447\n",
            "loss after iteration 1100: 25.265605\n",
            "loss after iteration 1200: 23.376013\n",
            "loss after iteration 1300: 21.827917\n",
            "loss after iteration 1400: 20.534285\n",
            "loss after iteration 1500: 19.435186\n",
            "loss after iteration 1600: 18.488146\n",
            "loss after iteration 1700: 17.662304\n",
            "loss after iteration 1800: 16.934728\n",
            "loss after iteration 1900: 16.288036\n",
            "loss after iteration 2000: 15.708811\n",
            "loss after iteration 2100: 15.186524\n",
            "loss after iteration 2200: 14.712791\n",
            "loss after iteration 2300: 14.280845\n",
            "loss after iteration 2400: 13.885155\n",
            "loss after iteration 2500: 13.521154\n",
            "loss after iteration 2600: 13.185032\n",
            "loss after iteration 2700: 12.873579\n",
            "loss after iteration 2800: 12.584074\n",
            "loss after iteration 2900: 12.314189\n",
            "loss after iteration 3000: 12.061921\n",
            "loss after iteration 3100: 11.825534\n",
            "loss after iteration 3200: 11.603517\n",
            "loss after iteration 3300: 11.394548\n",
            "loss after iteration 3400: 11.197462\n",
            "loss after iteration 3500: 11.011233\n",
            "loss after iteration 3600: 10.834947\n",
            "loss after iteration 3700: 10.667792\n",
            "loss after iteration 3800: 10.509042\n",
            "loss after iteration 3900: 10.358046\n",
            "loss after iteration 4000: 10.214217\n",
            "loss after iteration 4100: 10.077028\n",
            "loss after iteration 4200: 9.946000\n",
            "loss after iteration 4300: 9.820699\n",
            "loss after iteration 4400: 9.700732\n",
            "loss after iteration 4500: 9.585741\n",
            "loss after iteration 4600: 9.475397\n",
            "loss after iteration 4700: 9.369404\n",
            "loss after iteration 4800: 9.267490\n",
            "loss after iteration 4900: 9.169404\n",
            "loss after iteration 5000: 9.074922\n",
            "loss after iteration 5100: 8.983834\n",
            "loss after iteration 5200: 8.895951\n",
            "loss after iteration 5300: 8.811102\n",
            "loss after iteration 5400: 8.729130\n",
            "loss after iteration 5500: 8.649890\n",
            "loss after iteration 5600: 8.573255\n",
            "loss after iteration 5700: 8.499105\n",
            "loss after iteration 5800: 8.427334\n",
            "loss after iteration 5900: 8.357845\n",
            "loss after iteration 6000: 8.290550\n",
            "loss after iteration 6100: 8.225368\n",
            "loss after iteration 6200: 8.162226\n",
            "loss after iteration 6300: 8.101058\n",
            "loss after iteration 6400: 8.041799\n",
            "loss after iteration 6500: 7.984394\n",
            "loss after iteration 6600: 7.928787\n",
            "loss after iteration 6700: 7.874927\n",
            "loss after iteration 6800: 7.822766\n",
            "loss after iteration 6900: 7.772257\n",
            "loss after iteration 7000: 7.723353\n",
            "loss after iteration 7100: 7.676010\n",
            "loss after iteration 7200: 7.630183\n",
            "loss after iteration 7300: 7.585828\n",
            "loss after iteration 7400: 7.542901\n",
            "loss after iteration 7500: 7.501357\n",
            "loss after iteration 7600: 7.461153\n",
            "loss after iteration 7700: 7.422243\n",
            "loss after iteration 7800: 7.384581\n",
            "loss after iteration 7900: 7.348122\n",
            "loss after iteration 8000: 7.312821\n",
            "loss after iteration 8100: 7.278632\n",
            "loss after iteration 8200: 7.245508\n",
            "loss after iteration 8300: 7.213405\n",
            "loss after iteration 8400: 7.182278\n",
            "loss after iteration 8500: 7.152084\n",
            "loss after iteration 8600: 7.122778\n",
            "loss after iteration 8700: 7.094320\n",
            "loss after iteration 8800: 7.066669\n",
            "loss after iteration 8900: 7.039787\n",
            "loss after iteration 9000: 7.013637\n",
            "loss after iteration 9100: 6.988183\n",
            "loss after iteration 9200: 6.963392\n",
            "loss after iteration 9300: 6.939234\n",
            "loss after iteration 9400: 6.915678\n",
            "loss after iteration 9500: 6.892698\n",
            "loss after iteration 9600: 6.870267\n",
            "loss after iteration 9700: 6.848363\n",
            "loss after iteration 9800: 6.826963\n",
            "loss after iteration 9900: 6.806048\n",
            "loss after iteration 10000: 6.785598\n",
            "loss after iteration 10100: 6.765597\n",
            "loss after iteration 10200: 6.746029\n",
            "loss after iteration 10300: 6.726880\n",
            "loss after iteration 10400: 6.708136\n",
            "loss after iteration 10500: 6.689785\n",
            "loss after iteration 10600: 6.671816\n",
            "loss after iteration 10700: 6.654218\n",
            "loss after iteration 10800: 6.636982\n",
            "loss after iteration 10900: 6.620098\n",
            "loss after iteration 11000: 6.603559\n",
            "loss after iteration 11100: 6.587356\n",
            "loss after iteration 11200: 6.571483\n",
            "loss after iteration 11300: 6.555931\n",
            "loss after iteration 11400: 6.540696\n",
            "loss after iteration 11500: 6.525770\n",
            "loss after iteration 11600: 6.511148\n",
            "loss after iteration 11700: 6.496823\n",
            "loss after iteration 11800: 6.482791\n",
            "loss after iteration 11900: 6.469047\n",
            "loss after iteration 12000: 6.455585\n",
            "loss after iteration 12100: 6.442401\n",
            "loss after iteration 12200: 6.429490\n",
            "loss after iteration 12300: 6.416847\n",
            "loss after iteration 12400: 6.404469\n",
            "loss after iteration 12500: 6.392350\n",
            "loss after iteration 12600: 6.380487\n",
            "loss after iteration 12700: 6.368876\n",
            "loss after iteration 12800: 6.357513\n",
            "loss after iteration 12900: 6.346395\n",
            "loss after iteration 13000: 6.335517\n",
            "loss after iteration 13100: 6.324875\n",
            "loss after iteration 13200: 6.314468\n",
            "loss after iteration 13300: 6.304290\n",
            "loss after iteration 13400: 6.294339\n",
            "loss after iteration 13500: 6.284612\n",
            "loss after iteration 13600: 6.275105\n",
            "loss after iteration 13700: 6.265815\n",
            "loss after iteration 13800: 6.256739\n",
            "loss after iteration 13900: 6.247874\n",
            "loss after iteration 14000: 6.239217\n",
            "loss after iteration 14100: 6.230764\n",
            "loss after iteration 14200: 6.222513\n",
            "loss after iteration 14300: 6.214459\n",
            "loss after iteration 14400: 6.206600\n",
            "loss after iteration 14500: 6.198932\n",
            "loss after iteration 14600: 6.191451\n",
            "loss after iteration 14700: 6.184154\n",
            "loss after iteration 14800: 6.177035\n",
            "loss after iteration 14900: 6.170092\n",
            "loss after iteration 15000: 6.163318\n",
            "loss after iteration 15100: 6.156710\n",
            "loss after iteration 15200: 6.150261\n",
            "loss after iteration 15300: 6.143967\n",
            "loss after iteration 15400: 6.137821\n",
            "loss after iteration 15500: 6.131818\n",
            "loss after iteration 15600: 6.125951\n",
            "loss after iteration 15700: 6.120212\n",
            "loss after iteration 15800: 6.114597\n",
            "loss after iteration 15900: 6.109096\n",
            "loss after iteration 16000: 6.103704\n",
            "loss after iteration 16100: 6.098411\n",
            "loss after iteration 16200: 6.093212\n",
            "loss after iteration 16300: 6.088097\n",
            "loss after iteration 16400: 6.083060\n",
            "loss after iteration 16500: 6.078092\n",
            "loss after iteration 16600: 6.073185\n",
            "loss after iteration 16700: 6.068331\n",
            "loss after iteration 16800: 6.063523\n",
            "loss after iteration 16900: 6.058752\n",
            "loss after iteration 17000: 6.054011\n",
            "loss after iteration 17100: 6.049292\n",
            "loss after iteration 17200: 6.044586\n",
            "loss after iteration 17300: 6.039886\n",
            "loss after iteration 17400: 6.035184\n",
            "loss after iteration 17500: 6.030473\n",
            "loss after iteration 17600: 6.025746\n",
            "loss after iteration 17700: 6.020995\n",
            "loss after iteration 17800: 6.016214\n",
            "loss after iteration 17900: 6.011395\n",
            "loss after iteration 18000: 6.006534\n",
            "loss after iteration 18100: 6.001623\n",
            "loss after iteration 18200: 5.996657\n",
            "loss after iteration 18300: 5.991633\n",
            "loss after iteration 18400: 5.986544\n",
            "loss after iteration 18500: 5.981387\n",
            "loss after iteration 18600: 5.976158\n",
            "loss after iteration 18700: 5.970856\n",
            "loss after iteration 18800: 5.965476\n",
            "loss after iteration 18900: 5.960018\n",
            "loss after iteration 19000: 5.954479\n",
            "loss after iteration 19100: 5.948860\n",
            "loss after iteration 19200: 5.943159\n",
            "loss after iteration 19300: 5.937376\n",
            "loss after iteration 19400: 5.931513\n",
            "loss after iteration 19500: 5.925569\n",
            "loss after iteration 19600: 5.919545\n",
            "loss after iteration 19700: 5.913444\n",
            "loss after iteration 19800: 5.907267\n",
            "loss after iteration 19900: 5.901015\n",
            "loss after iteration 20000: 5.894691\n",
            "loss after iteration 20100: 5.888296\n",
            "loss after iteration 20200: 5.881834\n",
            "loss after iteration 20300: 5.875307\n",
            "loss after iteration 20400: 5.868717\n",
            "loss after iteration 20500: 5.862066\n",
            "loss after iteration 20600: 5.855358\n",
            "loss after iteration 20700: 5.848594\n",
            "loss after iteration 20800: 5.841779\n",
            "loss after iteration 20900: 5.834913\n",
            "loss after iteration 21000: 5.827999\n",
            "loss after iteration 21100: 5.821040\n",
            "loss after iteration 21200: 5.814039\n",
            "loss after iteration 21300: 5.806997\n",
            "loss after iteration 21400: 5.799917\n",
            "loss after iteration 21500: 5.792801\n",
            "loss after iteration 21600: 5.785650\n",
            "loss after iteration 21700: 5.778468\n",
            "loss after iteration 21800: 5.771255\n",
            "loss after iteration 21900: 5.764014\n",
            "loss after iteration 22000: 5.756746\n",
            "loss after iteration 22100: 5.749452\n",
            "loss after iteration 22200: 5.742135\n",
            "loss after iteration 22300: 5.734795\n",
            "loss after iteration 22400: 5.727434\n",
            "loss after iteration 22500: 5.720053\n",
            "loss after iteration 22600: 5.712652\n",
            "loss after iteration 22700: 5.705234\n",
            "loss after iteration 22800: 5.697798\n",
            "loss after iteration 22900: 5.690346\n",
            "loss after iteration 23000: 5.682879\n",
            "loss after iteration 23100: 5.675396\n",
            "loss after iteration 23200: 5.667899\n",
            "loss after iteration 23300: 5.660388\n",
            "loss after iteration 23400: 5.652864\n",
            "loss after iteration 23500: 5.645326\n",
            "loss after iteration 23600: 5.637776\n",
            "loss after iteration 23700: 5.630213\n",
            "loss after iteration 23800: 5.622638\n",
            "loss after iteration 23900: 5.615050\n",
            "loss after iteration 24000: 5.607451\n",
            "loss after iteration 24100: 5.599839\n",
            "loss after iteration 24200: 5.592214\n",
            "loss after iteration 24300: 5.584578\n",
            "loss after iteration 24400: 5.576929\n",
            "loss after iteration 24500: 5.569267\n",
            "loss after iteration 24600: 5.561592\n",
            "loss after iteration 24700: 5.553904\n",
            "loss after iteration 24800: 5.546203\n",
            "loss after iteration 24900: 5.538488\n",
            "loss after iteration 25000: 5.530758\n",
            "loss after iteration 25100: 5.523014\n",
            "loss after iteration 25200: 5.515255\n",
            "loss after iteration 25300: 5.507480\n",
            "loss after iteration 25400: 5.499689\n",
            "loss after iteration 25500: 5.491880\n",
            "loss after iteration 25600: 5.484055\n",
            "loss after iteration 25700: 5.476211\n",
            "loss after iteration 25800: 5.468348\n",
            "loss after iteration 25900: 5.460465\n",
            "loss after iteration 26000: 5.452562\n",
            "loss after iteration 26100: 5.444638\n",
            "loss after iteration 26200: 5.436692\n",
            "loss after iteration 26300: 5.428723\n",
            "loss after iteration 26400: 5.420729\n",
            "loss after iteration 26500: 5.412711\n",
            "loss after iteration 26600: 5.404668\n",
            "loss after iteration 26700: 5.396597\n",
            "loss after iteration 26800: 5.388499\n",
            "loss after iteration 26900: 5.380372\n",
            "loss after iteration 27000: 5.372215\n",
            "loss after iteration 27100: 5.364027\n",
            "loss after iteration 27200: 5.355807\n",
            "loss after iteration 27300: 5.347555\n",
            "loss after iteration 27400: 5.339269\n",
            "loss after iteration 27500: 5.330948\n",
            "loss after iteration 27600: 5.322591\n",
            "loss after iteration 27700: 5.314199\n",
            "loss after iteration 27800: 5.305768\n",
            "loss after iteration 27900: 5.297300\n",
            "loss after iteration 28000: 5.288794\n",
            "loss after iteration 28100: 5.280248\n",
            "loss after iteration 28200: 5.271662\n",
            "loss after iteration 28300: 5.263037\n",
            "loss after iteration 28400: 5.254371\n",
            "loss after iteration 28500: 5.245665\n",
            "loss after iteration 28600: 5.236918\n",
            "loss after iteration 28700: 5.228130\n",
            "loss after iteration 28800: 5.219302\n",
            "loss after iteration 28900: 5.210434\n",
            "loss after iteration 29000: 5.201527\n",
            "loss after iteration 29100: 5.192579\n",
            "loss after iteration 29200: 5.183594\n",
            "loss after iteration 29300: 5.174569\n",
            "loss after iteration 29400: 5.165508\n",
            "loss after iteration 29500: 5.156409\n",
            "loss after iteration 29600: 5.147274\n",
            "loss after iteration 29700: 5.138104\n",
            "loss after iteration 29800: 5.128900\n",
            "loss after iteration 29900: 5.119662\n",
            "loss after iteration 30000: 5.110391\n",
            "loss after iteration 30100: 5.101088\n",
            "loss after iteration 30200: 5.091754\n",
            "loss after iteration 30300: 5.082390\n",
            "loss after iteration 30400: 5.072996\n",
            "loss after iteration 30500: 5.063573\n",
            "loss after iteration 30600: 5.054122\n",
            "loss after iteration 30700: 5.044644\n",
            "loss after iteration 30800: 5.035139\n",
            "loss after iteration 30900: 5.025608\n",
            "loss after iteration 31000: 5.016052\n",
            "loss after iteration 31100: 5.006470\n",
            "loss after iteration 31200: 4.996864\n",
            "loss after iteration 31300: 4.987233\n",
            "loss after iteration 31400: 4.977579\n",
            "loss after iteration 31500: 4.967902\n",
            "loss after iteration 31600: 4.958202\n",
            "loss after iteration 31700: 4.948479\n",
            "loss after iteration 31800: 4.938734\n",
            "loss after iteration 31900: 4.928967\n",
            "loss after iteration 32000: 4.919178\n",
            "loss after iteration 32100: 4.909368\n",
            "loss after iteration 32200: 4.899537\n",
            "loss after iteration 32300: 4.889684\n",
            "loss after iteration 32400: 4.879811\n",
            "loss after iteration 32500: 4.869918\n",
            "loss after iteration 32600: 4.860004\n",
            "loss after iteration 32700: 4.850070\n",
            "loss after iteration 32800: 4.840116\n",
            "loss after iteration 32900: 4.830143\n",
            "loss after iteration 33000: 4.820150\n",
            "loss after iteration 33100: 4.810137\n",
            "loss after iteration 33200: 4.800106\n",
            "loss after iteration 33300: 4.790056\n",
            "loss after iteration 33400: 4.779987\n",
            "loss after iteration 33500: 4.769900\n",
            "loss after iteration 33600: 4.759795\n",
            "loss after iteration 33700: 4.749672\n",
            "loss after iteration 33800: 4.739531\n",
            "loss after iteration 33900: 4.729373\n",
            "loss after iteration 34000: 4.719198\n",
            "loss after iteration 34100: 4.709006\n",
            "loss after iteration 34200: 4.698798\n",
            "loss after iteration 34300: 4.688573\n",
            "loss after iteration 34400: 4.678333\n",
            "loss after iteration 34500: 4.668077\n",
            "loss after iteration 34600: 4.657806\n",
            "loss after iteration 34700: 4.647520\n",
            "loss after iteration 34800: 4.637219\n",
            "loss after iteration 34900: 4.626905\n",
            "loss after iteration 35000: 4.616577\n",
            "loss after iteration 35100: 4.606235\n",
            "loss after iteration 35200: 4.595881\n",
            "loss after iteration 35300: 4.585515\n",
            "loss after iteration 35400: 4.575136\n",
            "loss after iteration 35500: 4.564747\n",
            "loss after iteration 35600: 4.554346\n",
            "loss after iteration 35700: 4.543935\n",
            "loss after iteration 35800: 4.533515\n",
            "loss after iteration 35900: 4.523085\n",
            "loss after iteration 36000: 4.512647\n",
            "loss after iteration 36100: 4.502200\n",
            "loss after iteration 36200: 4.491746\n",
            "loss after iteration 36300: 4.481286\n",
            "loss after iteration 36400: 4.470819\n",
            "loss after iteration 36500: 4.460347\n",
            "loss after iteration 36600: 4.449870\n",
            "loss after iteration 36700: 4.439389\n",
            "loss after iteration 36800: 4.428905\n",
            "loss after iteration 36900: 4.418418\n",
            "loss after iteration 37000: 4.407929\n",
            "loss after iteration 37100: 4.397439\n",
            "loss after iteration 37200: 4.386949\n",
            "loss after iteration 37300: 4.376459\n",
            "loss after iteration 37400: 4.365970\n",
            "loss after iteration 37500: 4.355483\n",
            "loss after iteration 37600: 4.345000\n",
            "loss after iteration 37700: 4.334519\n",
            "loss after iteration 37800: 4.324043\n",
            "loss after iteration 37900: 4.313572\n",
            "loss after iteration 38000: 4.303107\n",
            "loss after iteration 38100: 4.292649\n",
            "loss after iteration 38200: 4.282198\n",
            "loss after iteration 38300: 4.271755\n",
            "loss after iteration 38400: 4.261322\n",
            "loss after iteration 38500: 4.250898\n",
            "loss after iteration 38600: 4.240485\n",
            "loss after iteration 38700: 4.230083\n",
            "loss after iteration 38800: 4.219693\n",
            "loss after iteration 38900: 4.209316\n",
            "loss after iteration 39000: 4.198953\n",
            "loss after iteration 39100: 4.188603\n",
            "loss after iteration 39200: 4.178268\n",
            "loss after iteration 39300: 4.167949\n",
            "loss after iteration 39400: 4.157646\n",
            "loss after iteration 39500: 4.147360\n",
            "loss after iteration 39600: 4.137091\n",
            "loss after iteration 39700: 4.126840\n",
            "loss after iteration 39800: 4.116607\n",
            "loss after iteration 39900: 4.106393\n",
            "loss after iteration 40000: 4.096199\n",
            "loss after iteration 40100: 4.086024\n",
            "loss after iteration 40200: 4.075870\n",
            "loss after iteration 40300: 4.065737\n",
            "loss after iteration 40400: 4.055625\n",
            "loss after iteration 40500: 4.045535\n",
            "loss after iteration 40600: 4.035467\n",
            "loss after iteration 40700: 4.025421\n",
            "loss after iteration 40800: 4.015398\n",
            "loss after iteration 40900: 4.005398\n",
            "loss after iteration 41000: 3.995421\n",
            "loss after iteration 41100: 3.985468\n",
            "loss after iteration 41200: 3.975539\n",
            "loss after iteration 41300: 3.965634\n",
            "loss after iteration 41400: 3.955753\n",
            "loss after iteration 41500: 3.945896\n",
            "loss after iteration 41600: 3.936064\n",
            "loss after iteration 41700: 3.926257\n",
            "loss after iteration 41800: 3.916475\n",
            "loss after iteration 41900: 3.906718\n",
            "loss after iteration 42000: 3.896986\n",
            "loss after iteration 42100: 3.887280\n",
            "loss after iteration 42200: 3.877599\n",
            "loss after iteration 42300: 3.867943\n",
            "loss after iteration 42400: 3.858313\n",
            "loss after iteration 42500: 3.848709\n",
            "loss after iteration 42600: 3.839130\n",
            "loss after iteration 42700: 3.829577\n",
            "loss after iteration 42800: 3.820050\n",
            "loss after iteration 42900: 3.810548\n",
            "loss after iteration 43000: 3.801072\n",
            "loss after iteration 43100: 3.791622\n",
            "loss after iteration 43200: 3.782197\n",
            "loss after iteration 43300: 3.772799\n",
            "loss after iteration 43400: 3.763426\n",
            "loss after iteration 43500: 3.754078\n",
            "loss after iteration 43600: 3.744757\n",
            "loss after iteration 43700: 3.735461\n",
            "loss after iteration 43800: 3.726191\n",
            "loss after iteration 43900: 3.716946\n",
            "loss after iteration 44000: 3.707727\n",
            "loss after iteration 44100: 3.698533\n",
            "loss after iteration 44200: 3.689365\n",
            "loss after iteration 44300: 3.680223\n",
            "loss after iteration 44400: 3.671105\n",
            "loss after iteration 44500: 3.662014\n",
            "loss after iteration 44600: 3.652947\n",
            "loss after iteration 44700: 3.643906\n",
            "loss after iteration 44800: 3.634890\n",
            "loss after iteration 44900: 3.625899\n",
            "loss after iteration 45000: 3.616934\n",
            "loss after iteration 45100: 3.607993\n",
            "loss after iteration 45200: 3.599078\n",
            "loss after iteration 45300: 3.590188\n",
            "loss after iteration 45400: 3.581322\n",
            "loss after iteration 45500: 3.572482\n",
            "loss after iteration 45600: 3.563667\n",
            "loss after iteration 45700: 3.554876\n",
            "loss after iteration 45800: 3.546111\n",
            "loss after iteration 45900: 3.537370\n",
            "loss after iteration 46000: 3.528654\n",
            "loss after iteration 46100: 3.519962\n",
            "loss after iteration 46200: 3.511296\n",
            "loss after iteration 46300: 3.502654\n",
            "loss after iteration 46400: 3.494036\n",
            "loss after iteration 46500: 3.485444\n",
            "loss after iteration 46600: 3.476876\n",
            "loss after iteration 46700: 3.468332\n",
            "loss after iteration 46800: 3.459813\n",
            "loss after iteration 46900: 3.451318\n",
            "loss after iteration 47000: 3.442848\n",
            "loss after iteration 47100: 3.434403\n",
            "loss after iteration 47200: 3.425981\n",
            "loss after iteration 47300: 3.417584\n",
            "loss after iteration 47400: 3.409212\n",
            "loss after iteration 47500: 3.400864\n",
            "loss after iteration 47600: 3.392540\n",
            "loss after iteration 47700: 3.384240\n",
            "loss after iteration 47800: 3.375965\n",
            "loss after iteration 47900: 3.367713\n",
            "loss after iteration 48000: 3.359486\n",
            "loss after iteration 48100: 3.351284\n",
            "loss after iteration 48200: 3.343105\n",
            "loss after iteration 48300: 3.334950\n",
            "loss after iteration 48400: 3.326820\n",
            "loss after iteration 48500: 3.318713\n",
            "loss after iteration 48600: 3.310631\n",
            "loss after iteration 48700: 3.302573\n",
            "loss after iteration 48800: 3.294538\n",
            "loss after iteration 48900: 3.286528\n",
            "loss after iteration 49000: 3.278541\n",
            "loss after iteration 49100: 3.270578\n",
            "loss after iteration 49200: 3.262639\n",
            "loss after iteration 49300: 3.254724\n",
            "loss after iteration 49400: 3.246833\n",
            "loss after iteration 49500: 3.238965\n",
            "loss after iteration 49600: 3.231121\n",
            "loss after iteration 49700: 3.223301\n",
            "loss after iteration 49800: 3.215504\n",
            "loss after iteration 49900: 3.207731\n",
            "loss after iteration 50000: 3.199982\n",
            "loss after iteration 50100: 3.192256\n",
            "loss after iteration 50200: 3.184553\n",
            "loss after iteration 50300: 3.176874\n",
            "loss after iteration 50400: 3.169218\n",
            "loss after iteration 50500: 3.161586\n",
            "loss after iteration 50600: 3.153976\n",
            "loss after iteration 50700: 3.146391\n",
            "loss after iteration 50800: 3.138828\n",
            "loss after iteration 50900: 3.131288\n",
            "loss after iteration 51000: 3.123771\n",
            "loss after iteration 51100: 3.116278\n",
            "loss after iteration 51200: 3.108807\n",
            "loss after iteration 51300: 3.101360\n",
            "loss after iteration 51400: 3.093935\n",
            "loss after iteration 51500: 3.086533\n",
            "loss after iteration 51600: 3.079153\n",
            "loss after iteration 51700: 3.071797\n",
            "loss after iteration 51800: 3.064463\n",
            "loss after iteration 51900: 3.057151\n",
            "loss after iteration 52000: 3.049862\n",
            "loss after iteration 52100: 3.042596\n",
            "loss after iteration 52200: 3.035351\n",
            "loss after iteration 52300: 3.028129\n",
            "loss after iteration 52400: 3.020930\n",
            "loss after iteration 52500: 3.013752\n",
            "loss after iteration 52600: 3.006597\n",
            "loss after iteration 52700: 2.999463\n",
            "loss after iteration 52800: 2.992352\n",
            "loss after iteration 52900: 2.985262\n",
            "loss after iteration 53000: 2.978195\n",
            "loss after iteration 53100: 2.971148\n",
            "loss after iteration 53200: 2.964124\n",
            "loss after iteration 53300: 2.957121\n",
            "loss after iteration 53400: 2.950140\n",
            "loss after iteration 53500: 2.943180\n",
            "loss after iteration 53600: 2.936241\n",
            "loss after iteration 53700: 2.929324\n",
            "loss after iteration 53800: 2.922428\n",
            "loss after iteration 53900: 2.915553\n",
            "loss after iteration 54000: 2.908699\n",
            "loss after iteration 54100: 2.901866\n",
            "loss after iteration 54200: 2.895054\n",
            "loss after iteration 54300: 2.888262\n",
            "loss after iteration 54400: 2.881492\n",
            "loss after iteration 54500: 2.874742\n",
            "loss after iteration 54600: 2.868012\n",
            "loss after iteration 54700: 2.861303\n",
            "loss after iteration 54800: 2.854614\n",
            "loss after iteration 54900: 2.847946\n",
            "loss after iteration 55000: 2.841298\n",
            "loss after iteration 55100: 2.834670\n",
            "loss after iteration 55200: 2.828062\n",
            "loss after iteration 55300: 2.821474\n",
            "loss after iteration 55400: 2.814905\n",
            "loss after iteration 55500: 2.808357\n",
            "loss after iteration 55600: 2.801828\n",
            "loss after iteration 55700: 2.795319\n",
            "loss after iteration 55800: 2.788829\n",
            "loss after iteration 55900: 2.782359\n",
            "loss after iteration 56000: 2.775908\n",
            "loss after iteration 56100: 2.769476\n",
            "loss after iteration 56200: 2.763064\n",
            "loss after iteration 56300: 2.756671\n",
            "loss after iteration 56400: 2.750296\n",
            "loss after iteration 56500: 2.743941\n",
            "loss after iteration 56600: 2.737604\n",
            "loss after iteration 56700: 2.731286\n",
            "loss after iteration 56800: 2.724987\n",
            "loss after iteration 56900: 2.718707\n",
            "loss after iteration 57000: 2.712445\n",
            "loss after iteration 57100: 2.706201\n",
            "loss after iteration 57200: 2.699975\n",
            "loss after iteration 57300: 2.693768\n",
            "loss after iteration 57400: 2.687579\n",
            "loss after iteration 57500: 2.681408\n",
            "loss after iteration 57600: 2.675255\n",
            "loss after iteration 57700: 2.669120\n",
            "loss after iteration 57800: 2.663003\n",
            "loss after iteration 57900: 2.656904\n",
            "loss after iteration 58000: 2.650822\n",
            "loss after iteration 58100: 2.644758\n",
            "loss after iteration 58200: 2.638711\n",
            "loss after iteration 58300: 2.632682\n",
            "loss after iteration 58400: 2.626670\n",
            "loss after iteration 58500: 2.620675\n",
            "loss after iteration 58600: 2.614698\n",
            "loss after iteration 58700: 2.608737\n",
            "loss after iteration 58800: 2.602794\n",
            "loss after iteration 58900: 2.596867\n",
            "loss after iteration 59000: 2.590958\n",
            "loss after iteration 59100: 2.585065\n",
            "loss after iteration 59200: 2.579189\n",
            "loss after iteration 59300: 2.573330\n",
            "loss after iteration 59400: 2.567487\n",
            "loss after iteration 59500: 2.561660\n",
            "loss after iteration 59600: 2.555850\n",
            "loss after iteration 59700: 2.550057\n",
            "loss after iteration 59800: 2.544280\n",
            "loss after iteration 59900: 2.538519\n",
            "loss after iteration 60000: 2.532774\n",
            "loss after iteration 60100: 2.527045\n",
            "loss after iteration 60200: 2.521332\n",
            "loss after iteration 60300: 2.515635\n",
            "loss after iteration 60400: 2.509954\n",
            "loss after iteration 60500: 2.504289\n",
            "loss after iteration 60600: 2.498639\n",
            "loss after iteration 60700: 2.493005\n",
            "loss after iteration 60800: 2.487387\n",
            "loss after iteration 60900: 2.481784\n",
            "loss after iteration 61000: 2.476197\n",
            "loss after iteration 61100: 2.470625\n",
            "loss after iteration 61200: 2.465068\n",
            "loss after iteration 61300: 2.459527\n",
            "loss after iteration 61400: 2.454001\n",
            "loss after iteration 61500: 2.448490\n",
            "loss after iteration 61600: 2.442994\n",
            "loss after iteration 61700: 2.437513\n",
            "loss after iteration 61800: 2.432047\n",
            "loss after iteration 61900: 2.426597\n",
            "loss after iteration 62000: 2.421160\n",
            "loss after iteration 62100: 2.415739\n",
            "loss after iteration 62200: 2.410332\n",
            "loss after iteration 62300: 2.404940\n",
            "loss after iteration 62400: 2.399563\n",
            "loss after iteration 62500: 2.394200\n",
            "loss after iteration 62600: 2.388852\n",
            "loss after iteration 62700: 2.383518\n",
            "loss after iteration 62800: 2.378198\n",
            "loss after iteration 62900: 2.372893\n",
            "loss after iteration 63000: 2.367602\n",
            "loss after iteration 63100: 2.362325\n",
            "loss after iteration 63200: 2.357063\n",
            "loss after iteration 63300: 2.351814\n",
            "loss after iteration 63400: 2.346580\n",
            "loss after iteration 63500: 2.341359\n",
            "loss after iteration 63600: 2.336153\n",
            "loss after iteration 63700: 2.330960\n",
            "loss after iteration 63800: 2.325781\n",
            "loss after iteration 63900: 2.320616\n",
            "loss after iteration 64000: 2.315465\n",
            "loss after iteration 64100: 2.310327\n",
            "loss after iteration 64200: 2.305203\n",
            "loss after iteration 64300: 2.300093\n",
            "loss after iteration 64400: 2.294996\n",
            "loss after iteration 64500: 2.289912\n",
            "loss after iteration 64600: 2.284842\n",
            "loss after iteration 64700: 2.279786\n",
            "loss after iteration 64800: 2.274742\n",
            "loss after iteration 64900: 2.269712\n",
            "loss after iteration 65000: 2.264696\n",
            "loss after iteration 65100: 2.259692\n",
            "loss after iteration 65200: 2.254701\n",
            "loss after iteration 65300: 2.249724\n",
            "loss after iteration 65400: 2.244759\n",
            "loss after iteration 65500: 2.239808\n",
            "loss after iteration 65600: 2.234869\n",
            "loss after iteration 65700: 2.229944\n",
            "loss after iteration 65800: 2.225031\n",
            "loss after iteration 65900: 2.220131\n",
            "loss after iteration 66000: 2.215244\n",
            "loss after iteration 66100: 2.210369\n",
            "loss after iteration 66200: 2.205507\n",
            "loss after iteration 66300: 2.200658\n",
            "loss after iteration 66400: 2.195821\n",
            "loss after iteration 66500: 2.190997\n",
            "loss after iteration 66600: 2.186185\n",
            "loss after iteration 66700: 2.181385\n",
            "loss after iteration 66800: 2.176598\n",
            "loss after iteration 66900: 2.171823\n",
            "loss after iteration 67000: 2.167061\n",
            "loss after iteration 67100: 2.162311\n",
            "loss after iteration 67200: 2.157573\n",
            "loss after iteration 67300: 2.152847\n",
            "loss after iteration 67400: 2.148133\n",
            "loss after iteration 67500: 2.143431\n",
            "loss after iteration 67600: 2.138741\n",
            "loss after iteration 67700: 2.134063\n",
            "loss after iteration 67800: 2.129397\n",
            "loss after iteration 67900: 2.124743\n",
            "loss after iteration 68000: 2.120101\n",
            "loss after iteration 68100: 2.115471\n",
            "loss after iteration 68200: 2.110852\n",
            "loss after iteration 68300: 2.106245\n",
            "loss after iteration 68400: 2.101650\n",
            "loss after iteration 68500: 2.097066\n",
            "loss after iteration 68600: 2.092494\n",
            "loss after iteration 68700: 2.087934\n",
            "loss after iteration 68800: 2.083385\n",
            "loss after iteration 68900: 2.078847\n",
            "loss after iteration 69000: 2.074321\n",
            "loss after iteration 69100: 2.069806\n",
            "loss after iteration 69200: 2.065302\n",
            "loss after iteration 69300: 2.060810\n",
            "loss after iteration 69400: 2.056329\n",
            "loss after iteration 69500: 2.051859\n",
            "loss after iteration 69600: 2.047401\n",
            "loss after iteration 69700: 2.042953\n",
            "loss after iteration 69800: 2.038517\n",
            "loss after iteration 69900: 2.034092\n",
            "loss after iteration 70000: 2.029677\n",
            "loss after iteration 70100: 2.025274\n",
            "loss after iteration 70200: 2.020882\n",
            "loss after iteration 70300: 2.016500\n",
            "loss after iteration 70400: 2.012129\n",
            "loss after iteration 70500: 2.007770\n",
            "loss after iteration 70600: 2.003420\n",
            "loss after iteration 70700: 1.999082\n",
            "loss after iteration 70800: 1.994755\n",
            "loss after iteration 70900: 1.990438\n",
            "loss after iteration 71000: 1.986131\n",
            "loss after iteration 71100: 1.981836\n",
            "loss after iteration 71200: 1.977551\n",
            "loss after iteration 71300: 1.973276\n",
            "loss after iteration 71400: 1.969012\n",
            "loss after iteration 71500: 1.964758\n",
            "loss after iteration 71600: 1.960515\n",
            "loss after iteration 71700: 1.956282\n",
            "loss after iteration 71800: 1.952060\n",
            "loss after iteration 71900: 1.947848\n",
            "loss after iteration 72000: 1.943646\n",
            "loss after iteration 72100: 1.939455\n",
            "loss after iteration 72200: 1.935273\n",
            "loss after iteration 72300: 1.931102\n",
            "loss after iteration 72400: 1.926941\n",
            "loss after iteration 72500: 1.922791\n",
            "loss after iteration 72600: 1.918650\n",
            "loss after iteration 72700: 1.914519\n",
            "loss after iteration 72800: 1.910399\n",
            "loss after iteration 72900: 1.906288\n",
            "loss after iteration 73000: 1.902188\n",
            "loss after iteration 73100: 1.898097\n",
            "loss after iteration 73200: 1.894016\n",
            "loss after iteration 73300: 1.889946\n",
            "loss after iteration 73400: 1.885885\n",
            "loss after iteration 73500: 1.881834\n",
            "loss after iteration 73600: 1.877792\n",
            "loss after iteration 73700: 1.873761\n",
            "loss after iteration 73800: 1.869739\n",
            "loss after iteration 73900: 1.865727\n",
            "loss after iteration 74000: 1.861725\n",
            "loss after iteration 74100: 1.857732\n",
            "loss after iteration 74200: 1.853749\n",
            "loss after iteration 74300: 1.849776\n",
            "loss after iteration 74400: 1.845812\n",
            "loss after iteration 74500: 1.841858\n",
            "loss after iteration 74600: 1.837913\n",
            "loss after iteration 74700: 1.833978\n",
            "loss after iteration 74800: 1.830052\n",
            "loss after iteration 74900: 1.826136\n",
            "loss after iteration 75000: 1.822229\n",
            "loss after iteration 75100: 1.818332\n",
            "loss after iteration 75200: 1.814444\n",
            "loss after iteration 75300: 1.810566\n",
            "loss after iteration 75400: 1.806697\n",
            "loss after iteration 75500: 1.802837\n",
            "loss after iteration 75600: 1.798986\n",
            "loss after iteration 75700: 1.795145\n",
            "loss after iteration 75800: 1.791313\n",
            "loss after iteration 75900: 1.787490\n",
            "loss after iteration 76000: 1.783677\n",
            "loss after iteration 76100: 1.779872\n",
            "loss after iteration 76200: 1.776077\n",
            "loss after iteration 76300: 1.772291\n",
            "loss after iteration 76400: 1.768514\n",
            "loss after iteration 76500: 1.764747\n",
            "loss after iteration 76600: 1.760988\n",
            "loss after iteration 76700: 1.757238\n",
            "loss after iteration 76800: 1.753498\n",
            "loss after iteration 76900: 1.749766\n",
            "loss after iteration 77000: 1.746044\n",
            "loss after iteration 77100: 1.742330\n",
            "loss after iteration 77200: 1.738626\n",
            "loss after iteration 77300: 1.734930\n",
            "loss after iteration 77400: 1.731244\n",
            "loss after iteration 77500: 1.727566\n",
            "loss after iteration 77600: 1.723897\n",
            "loss after iteration 77700: 1.720238\n",
            "loss after iteration 77800: 1.716587\n",
            "loss after iteration 77900: 1.712944\n",
            "loss after iteration 78000: 1.709311\n",
            "loss after iteration 78100: 1.705687\n",
            "loss after iteration 78200: 1.702071\n",
            "loss after iteration 78300: 1.698464\n",
            "loss after iteration 78400: 1.694866\n",
            "loss after iteration 78500: 1.691276\n",
            "loss after iteration 78600: 1.687696\n",
            "loss after iteration 78700: 1.684124\n",
            "loss after iteration 78800: 1.680560\n",
            "loss after iteration 78900: 1.677006\n",
            "loss after iteration 79000: 1.673460\n",
            "loss after iteration 79100: 1.669923\n",
            "loss after iteration 79200: 1.666394\n",
            "loss after iteration 79300: 1.662874\n",
            "loss after iteration 79400: 1.659363\n",
            "loss after iteration 79500: 1.655860\n",
            "loss after iteration 79600: 1.652366\n",
            "loss after iteration 79700: 1.648880\n",
            "loss after iteration 79800: 1.645403\n",
            "loss after iteration 79900: 1.641934\n",
            "loss after iteration 80000: 1.638474\n",
            "loss after iteration 80100: 1.635022\n",
            "loss after iteration 80200: 1.631579\n",
            "loss after iteration 80300: 1.628145\n",
            "loss after iteration 80400: 1.624719\n",
            "loss after iteration 80500: 1.621301\n",
            "loss after iteration 80600: 1.617891\n",
            "loss after iteration 80700: 1.614491\n",
            "loss after iteration 80800: 1.611098\n",
            "loss after iteration 80900: 1.607714\n",
            "loss after iteration 81000: 1.604338\n",
            "loss after iteration 81100: 1.600971\n",
            "loss after iteration 81200: 1.597612\n",
            "loss after iteration 81300: 1.594261\n",
            "loss after iteration 81400: 1.590919\n",
            "loss after iteration 81500: 1.587585\n",
            "loss after iteration 81600: 1.584259\n",
            "loss after iteration 81700: 1.580941\n",
            "loss after iteration 81800: 1.577632\n",
            "loss after iteration 81900: 1.574331\n",
            "loss after iteration 82000: 1.571038\n",
            "loss after iteration 82100: 1.567753\n",
            "loss after iteration 82200: 1.564477\n",
            "loss after iteration 82300: 1.561209\n",
            "loss after iteration 82400: 1.557949\n",
            "loss after iteration 82500: 1.554697\n",
            "loss after iteration 82600: 1.551453\n",
            "loss after iteration 82700: 1.548218\n",
            "loss after iteration 82800: 1.544990\n",
            "loss after iteration 82900: 1.541771\n",
            "loss after iteration 83000: 1.538560\n",
            "loss after iteration 83100: 1.535356\n",
            "loss after iteration 83200: 1.532161\n",
            "loss after iteration 83300: 1.528974\n",
            "loss after iteration 83400: 1.525795\n",
            "loss after iteration 83500: 1.522624\n",
            "loss after iteration 83600: 1.519461\n",
            "loss after iteration 83700: 1.516306\n",
            "loss after iteration 83800: 1.513160\n",
            "loss after iteration 83900: 1.510021\n",
            "loss after iteration 84000: 1.506890\n",
            "loss after iteration 84100: 1.503766\n",
            "loss after iteration 84200: 1.500651\n",
            "loss after iteration 84300: 1.497544\n",
            "loss after iteration 84400: 1.494445\n",
            "loss after iteration 84500: 1.491353\n",
            "loss after iteration 84600: 1.488270\n",
            "loss after iteration 84700: 1.485194\n",
            "loss after iteration 84800: 1.482126\n",
            "loss after iteration 84900: 1.479067\n",
            "loss after iteration 85000: 1.476014\n",
            "loss after iteration 85100: 1.472970\n",
            "loss after iteration 85200: 1.469934\n",
            "loss after iteration 85300: 1.466905\n",
            "loss after iteration 85400: 1.463884\n",
            "loss after iteration 85500: 1.460871\n",
            "loss after iteration 85600: 1.457865\n",
            "loss after iteration 85700: 1.454868\n",
            "loss after iteration 85800: 1.451878\n",
            "loss after iteration 85900: 1.448895\n",
            "loss after iteration 86000: 1.445921\n",
            "loss after iteration 86100: 1.442954\n",
            "loss after iteration 86200: 1.439995\n",
            "loss after iteration 86300: 1.437043\n",
            "loss after iteration 86400: 1.434099\n",
            "loss after iteration 86500: 1.431163\n",
            "loss after iteration 86600: 1.428234\n",
            "loss after iteration 86700: 1.425313\n",
            "loss after iteration 86800: 1.422399\n",
            "loss after iteration 86900: 1.419493\n",
            "loss after iteration 87000: 1.416595\n",
            "loss after iteration 87100: 1.413704\n",
            "loss after iteration 87200: 1.410821\n",
            "loss after iteration 87300: 1.407945\n",
            "loss after iteration 87400: 1.405077\n",
            "loss after iteration 87500: 1.402216\n",
            "loss after iteration 87600: 1.399362\n",
            "loss after iteration 87700: 1.396516\n",
            "loss after iteration 87800: 1.393678\n",
            "loss after iteration 87900: 1.390847\n",
            "loss after iteration 88000: 1.388023\n",
            "loss after iteration 88100: 1.385207\n",
            "loss after iteration 88200: 1.382398\n",
            "loss after iteration 88300: 1.379597\n",
            "loss after iteration 88400: 1.376802\n",
            "loss after iteration 88500: 1.374016\n",
            "loss after iteration 88600: 1.371236\n",
            "loss after iteration 88700: 1.368464\n",
            "loss after iteration 88800: 1.365699\n",
            "loss after iteration 88900: 1.362941\n",
            "loss after iteration 89000: 1.360191\n",
            "loss after iteration 89100: 1.357448\n",
            "loss after iteration 89200: 1.354712\n",
            "loss after iteration 89300: 1.351984\n",
            "loss after iteration 89400: 1.349262\n",
            "loss after iteration 89500: 1.346548\n",
            "loss after iteration 89600: 1.343841\n",
            "loss after iteration 89700: 1.341141\n",
            "loss after iteration 89800: 1.338448\n",
            "loss after iteration 89900: 1.335763\n",
            "loss after iteration 90000: 1.333084\n",
            "loss after iteration 90100: 1.330413\n",
            "loss after iteration 90200: 1.327749\n",
            "loss after iteration 90300: 1.325092\n",
            "loss after iteration 90400: 1.322441\n",
            "loss after iteration 90500: 1.319798\n",
            "loss after iteration 90600: 1.317162\n",
            "loss after iteration 90700: 1.314533\n",
            "loss after iteration 90800: 1.311911\n",
            "loss after iteration 90900: 1.309296\n",
            "loss after iteration 91000: 1.306688\n",
            "loss after iteration 91100: 1.304087\n",
            "loss after iteration 91200: 1.301493\n",
            "loss after iteration 91300: 1.298905\n",
            "loss after iteration 91400: 1.296325\n",
            "loss after iteration 91500: 1.293752\n",
            "loss after iteration 91600: 1.291185\n",
            "loss after iteration 91700: 1.288625\n",
            "loss after iteration 91800: 1.286072\n",
            "loss after iteration 91900: 1.283526\n",
            "loss after iteration 92000: 1.280987\n",
            "loss after iteration 92100: 1.278455\n",
            "loss after iteration 92200: 1.275929\n",
            "loss after iteration 92300: 1.273410\n",
            "loss after iteration 92400: 1.270898\n",
            "loss after iteration 92500: 1.268393\n",
            "loss after iteration 92600: 1.265894\n",
            "loss after iteration 92700: 1.263402\n",
            "loss after iteration 92800: 1.260917\n",
            "loss after iteration 92900: 1.258438\n",
            "loss after iteration 93000: 1.255966\n",
            "loss after iteration 93100: 1.253501\n",
            "loss after iteration 93200: 1.251043\n",
            "loss after iteration 93300: 1.248591\n",
            "loss after iteration 93400: 1.246145\n",
            "loss after iteration 93500: 1.243706\n",
            "loss after iteration 93600: 1.241274\n",
            "loss after iteration 93700: 1.238848\n",
            "loss after iteration 93800: 1.236429\n",
            "loss after iteration 93900: 1.234016\n",
            "loss after iteration 94000: 1.231610\n",
            "loss after iteration 94100: 1.229211\n",
            "loss after iteration 94200: 1.226817\n",
            "loss after iteration 94300: 1.224431\n",
            "loss after iteration 94400: 1.222050\n",
            "loss after iteration 94500: 1.219677\n",
            "loss after iteration 94600: 1.217309\n",
            "loss after iteration 94700: 1.214948\n",
            "loss after iteration 94800: 1.212593\n",
            "loss after iteration 94900: 1.210245\n",
            "loss after iteration 95000: 1.207903\n",
            "loss after iteration 95100: 1.205568\n",
            "loss after iteration 95200: 1.203238\n",
            "loss after iteration 95300: 1.200915\n",
            "loss after iteration 95400: 1.198598\n",
            "loss after iteration 95500: 1.196288\n",
            "loss after iteration 95600: 1.193984\n",
            "loss after iteration 95700: 1.191686\n",
            "loss after iteration 95800: 1.189394\n",
            "loss after iteration 95900: 1.187109\n",
            "loss after iteration 96000: 1.184829\n",
            "loss after iteration 96100: 1.182556\n",
            "loss after iteration 96200: 1.180289\n",
            "loss after iteration 96300: 1.178028\n",
            "loss after iteration 96400: 1.175773\n",
            "loss after iteration 96500: 1.173525\n",
            "loss after iteration 96600: 1.171282\n",
            "loss after iteration 96700: 1.169046\n",
            "loss after iteration 96800: 1.166815\n",
            "loss after iteration 96900: 1.164591\n",
            "loss after iteration 97000: 1.162372\n",
            "loss after iteration 97100: 1.160160\n",
            "loss after iteration 97200: 1.157954\n",
            "loss after iteration 97300: 1.155754\n",
            "loss after iteration 97400: 1.153559\n",
            "loss after iteration 97500: 1.151371\n",
            "loss after iteration 97600: 1.149188\n",
            "loss after iteration 97700: 1.147012\n",
            "loss after iteration 97800: 1.144841\n",
            "loss after iteration 97900: 1.142676\n",
            "loss after iteration 98000: 1.140517\n",
            "loss after iteration 98100: 1.138364\n",
            "loss after iteration 98200: 1.136217\n",
            "loss after iteration 98300: 1.134076\n",
            "loss after iteration 98400: 1.131940\n",
            "loss after iteration 98500: 1.129810\n",
            "loss after iteration 98600: 1.127686\n",
            "loss after iteration 98700: 1.125568\n",
            "loss after iteration 98800: 1.123456\n",
            "loss after iteration 98900: 1.121349\n",
            "loss after iteration 99000: 1.119248\n",
            "loss after iteration 99100: 1.117153\n",
            "loss after iteration 99200: 1.115063\n",
            "loss after iteration 99300: 1.112979\n",
            "loss after iteration 99400: 1.110901\n",
            "loss after iteration 99500: 1.108828\n",
            "loss after iteration 99600: 1.106761\n",
            "loss after iteration 99700: 1.104699\n",
            "loss after iteration 99800: 1.102643\n",
            "loss after iteration 99900: 1.100593\n",
            "loss after iteration 100000: 1.098548\n",
            "loss after iteration 100100: 1.096509\n",
            "loss after iteration 100200: 1.094476\n",
            "loss after iteration 100300: 1.092447\n",
            "loss after iteration 100400: 1.090425\n",
            "loss after iteration 100500: 1.088408\n",
            "loss after iteration 100600: 1.086396\n",
            "loss after iteration 100700: 1.084390\n",
            "loss after iteration 100800: 1.082389\n",
            "loss after iteration 100900: 1.080393\n",
            "loss after iteration 101000: 1.078403\n",
            "loss after iteration 101100: 1.076419\n",
            "loss after iteration 101200: 1.074440\n",
            "loss after iteration 101300: 1.072466\n",
            "loss after iteration 101400: 1.070497\n",
            "loss after iteration 101500: 1.068534\n",
            "loss after iteration 101600: 1.066576\n",
            "loss after iteration 101700: 1.064623\n",
            "loss after iteration 101800: 1.062676\n",
            "loss after iteration 101900: 1.060734\n",
            "loss after iteration 102000: 1.058797\n",
            "loss after iteration 102100: 1.056866\n",
            "loss after iteration 102200: 1.054939\n",
            "loss after iteration 102300: 1.053018\n",
            "loss after iteration 102400: 1.051102\n",
            "loss after iteration 102500: 1.049192\n",
            "loss after iteration 102600: 1.047286\n",
            "loss after iteration 102700: 1.045385\n",
            "loss after iteration 102800: 1.043490\n",
            "loss after iteration 102900: 1.041600\n",
            "loss after iteration 103000: 1.039715\n",
            "loss after iteration 103100: 1.037835\n",
            "loss after iteration 103200: 1.035960\n",
            "loss after iteration 103300: 1.034090\n",
            "loss after iteration 103400: 1.032225\n",
            "loss after iteration 103500: 1.030365\n",
            "loss after iteration 103600: 1.028511\n",
            "loss after iteration 103700: 1.026661\n",
            "loss after iteration 103800: 1.024816\n",
            "loss after iteration 103900: 1.022976\n",
            "loss after iteration 104000: 1.021141\n",
            "loss after iteration 104100: 1.019311\n",
            "loss after iteration 104200: 1.017486\n",
            "loss after iteration 104300: 1.015666\n",
            "loss after iteration 104400: 1.013851\n",
            "loss after iteration 104500: 1.012040\n",
            "loss after iteration 104600: 1.010235\n",
            "loss after iteration 104700: 1.008434\n",
            "loss after iteration 104800: 1.006638\n",
            "loss after iteration 104900: 1.004848\n",
            "loss after iteration 105000: 1.003061\n",
            "loss after iteration 105100: 1.001280\n",
            "loss after iteration 105200: 0.999503\n",
            "loss after iteration 105300: 0.997732\n",
            "loss after iteration 105400: 0.995964\n",
            "loss after iteration 105500: 0.994202\n",
            "loss after iteration 105600: 0.992445\n",
            "loss after iteration 105700: 0.990692\n",
            "loss after iteration 105800: 0.988943\n",
            "loss after iteration 105900: 0.987200\n",
            "loss after iteration 106000: 0.985461\n",
            "loss after iteration 106100: 0.983727\n",
            "loss after iteration 106200: 0.981997\n",
            "loss after iteration 106300: 0.980272\n",
            "loss after iteration 106400: 0.978552\n",
            "loss after iteration 106500: 0.976836\n",
            "loss after iteration 106600: 0.975125\n",
            "loss after iteration 106700: 0.973418\n",
            "loss after iteration 106800: 0.971716\n",
            "loss after iteration 106900: 0.970019\n",
            "loss after iteration 107000: 0.968326\n",
            "loss after iteration 107100: 0.966637\n",
            "loss after iteration 107200: 0.964954\n",
            "loss after iteration 107300: 0.963274\n",
            "loss after iteration 107400: 0.961599\n",
            "loss after iteration 107500: 0.959928\n",
            "loss after iteration 107600: 0.958262\n",
            "loss after iteration 107700: 0.956601\n",
            "loss after iteration 107800: 0.954943\n",
            "loss after iteration 107900: 0.953291\n",
            "loss after iteration 108000: 0.951642\n",
            "loss after iteration 108100: 0.949998\n",
            "loss after iteration 108200: 0.948358\n",
            "loss after iteration 108300: 0.946723\n",
            "loss after iteration 108400: 0.945092\n",
            "loss after iteration 108500: 0.943465\n",
            "loss after iteration 108600: 0.941843\n",
            "loss after iteration 108700: 0.940225\n",
            "loss after iteration 108800: 0.938611\n",
            "loss after iteration 108900: 0.937002\n",
            "loss after iteration 109000: 0.935396\n",
            "loss after iteration 109100: 0.933795\n",
            "loss after iteration 109200: 0.932198\n",
            "loss after iteration 109300: 0.930606\n",
            "loss after iteration 109400: 0.929017\n",
            "loss after iteration 109500: 0.927433\n",
            "loss after iteration 109600: 0.925853\n",
            "loss after iteration 109700: 0.924277\n",
            "loss after iteration 109800: 0.922706\n",
            "loss after iteration 109900: 0.921138\n",
            "loss after iteration 110000: 0.919575\n",
            "loss after iteration 110100: 0.918016\n",
            "loss after iteration 110200: 0.916460\n",
            "loss after iteration 110300: 0.914909\n",
            "loss after iteration 110400: 0.913362\n",
            "loss after iteration 110500: 0.911819\n",
            "loss after iteration 110600: 0.910280\n",
            "loss after iteration 110700: 0.908745\n",
            "loss after iteration 110800: 0.907215\n",
            "loss after iteration 110900: 0.905688\n",
            "loss after iteration 111000: 0.904165\n",
            "loss after iteration 111100: 0.902646\n",
            "loss after iteration 111200: 0.901131\n",
            "loss after iteration 111300: 0.899620\n",
            "loss after iteration 111400: 0.898114\n",
            "loss after iteration 111500: 0.896611\n",
            "loss after iteration 111600: 0.895111\n",
            "loss after iteration 111700: 0.893616\n",
            "loss after iteration 111800: 0.892125\n",
            "loss after iteration 111900: 0.890638\n",
            "loss after iteration 112000: 0.889154\n",
            "loss after iteration 112100: 0.887675\n",
            "loss after iteration 112200: 0.886199\n",
            "loss after iteration 112300: 0.884727\n",
            "loss after iteration 112400: 0.883259\n",
            "loss after iteration 112500: 0.881795\n",
            "loss after iteration 112600: 0.880334\n",
            "loss after iteration 112700: 0.878878\n",
            "loss after iteration 112800: 0.877425\n",
            "loss after iteration 112900: 0.875976\n",
            "loss after iteration 113000: 0.874530\n",
            "loss after iteration 113100: 0.873089\n",
            "loss after iteration 113200: 0.871651\n",
            "loss after iteration 113300: 0.870217\n",
            "loss after iteration 113400: 0.868786\n",
            "loss after iteration 113500: 0.867360\n",
            "loss after iteration 113600: 0.865937\n",
            "loss after iteration 113700: 0.864517\n",
            "loss after iteration 113800: 0.863102\n",
            "loss after iteration 113900: 0.861690\n",
            "loss after iteration 114000: 0.860281\n",
            "loss after iteration 114100: 0.858877\n",
            "loss after iteration 114200: 0.857475\n",
            "loss after iteration 114300: 0.856078\n",
            "loss after iteration 114400: 0.854684\n",
            "loss after iteration 114500: 0.853294\n",
            "loss after iteration 114600: 0.851907\n",
            "loss after iteration 114700: 0.850524\n",
            "loss after iteration 114800: 0.849144\n",
            "loss after iteration 114900: 0.847768\n",
            "loss after iteration 115000: 0.846396\n",
            "loss after iteration 115100: 0.845027\n",
            "loss after iteration 115200: 0.843661\n",
            "loss after iteration 115300: 0.842299\n",
            "loss after iteration 115400: 0.840941\n",
            "loss after iteration 115500: 0.839586\n",
            "loss after iteration 115600: 0.838234\n",
            "loss after iteration 115700: 0.836886\n",
            "loss after iteration 115800: 0.835541\n",
            "loss after iteration 115900: 0.834200\n",
            "loss after iteration 116000: 0.832862\n",
            "loss after iteration 116100: 0.831528\n",
            "loss after iteration 116200: 0.830197\n",
            "loss after iteration 116300: 0.828869\n",
            "loss after iteration 116400: 0.827545\n",
            "loss after iteration 116500: 0.826224\n",
            "loss after iteration 116600: 0.824907\n",
            "loss after iteration 116700: 0.823593\n",
            "loss after iteration 116800: 0.822282\n",
            "loss after iteration 116900: 0.820974\n",
            "loss after iteration 117000: 0.819670\n",
            "loss after iteration 117100: 0.818369\n",
            "loss after iteration 117200: 0.817072\n",
            "loss after iteration 117300: 0.815778\n",
            "loss after iteration 117400: 0.814487\n",
            "loss after iteration 117500: 0.813199\n",
            "loss after iteration 117600: 0.811914\n",
            "loss after iteration 117700: 0.810633\n",
            "loss after iteration 117800: 0.809355\n",
            "loss after iteration 117900: 0.808080\n",
            "loss after iteration 118000: 0.806809\n",
            "loss after iteration 118100: 0.805541\n",
            "loss after iteration 118200: 0.804275\n",
            "loss after iteration 118300: 0.803014\n",
            "loss after iteration 118400: 0.801755\n",
            "loss after iteration 118500: 0.800499\n",
            "loss after iteration 118600: 0.799247\n",
            "loss after iteration 118700: 0.797997\n",
            "loss after iteration 118800: 0.796751\n",
            "loss after iteration 118900: 0.795508\n",
            "loss after iteration 119000: 0.794268\n",
            "loss after iteration 119100: 0.793031\n",
            "loss after iteration 119200: 0.791798\n",
            "loss after iteration 119300: 0.790567\n",
            "loss after iteration 119400: 0.789340\n",
            "loss after iteration 119500: 0.788115\n",
            "loss after iteration 119600: 0.786894\n",
            "loss after iteration 119700: 0.785675\n",
            "loss after iteration 119800: 0.784460\n",
            "loss after iteration 119900: 0.783248\n",
            "loss after iteration 120000: 0.782038\n",
            "loss after iteration 120100: 0.780832\n",
            "loss after iteration 120200: 0.779629\n",
            "loss after iteration 120300: 0.778428\n",
            "loss after iteration 120400: 0.777231\n",
            "loss after iteration 120500: 0.776037\n",
            "loss after iteration 120600: 0.774845\n",
            "loss after iteration 120700: 0.773657\n",
            "loss after iteration 120800: 0.772472\n",
            "loss after iteration 120900: 0.771289\n",
            "loss after iteration 121000: 0.770109\n",
            "loss after iteration 121100: 0.768933\n",
            "loss after iteration 121200: 0.767759\n",
            "loss after iteration 121300: 0.766588\n",
            "loss after iteration 121400: 0.765420\n",
            "loss after iteration 121500: 0.764255\n",
            "loss after iteration 121600: 0.763093\n",
            "loss after iteration 121700: 0.761933\n",
            "loss after iteration 121800: 0.760777\n",
            "loss after iteration 121900: 0.759623\n",
            "loss after iteration 122000: 0.758473\n",
            "loss after iteration 122100: 0.757325\n",
            "loss after iteration 122200: 0.756179\n",
            "loss after iteration 122300: 0.755037\n",
            "loss after iteration 122400: 0.753897\n",
            "loss after iteration 122500: 0.752761\n",
            "loss after iteration 122600: 0.751627\n",
            "loss after iteration 122700: 0.750496\n",
            "loss after iteration 122800: 0.749367\n",
            "loss after iteration 122900: 0.748241\n",
            "loss after iteration 123000: 0.747119\n",
            "loss after iteration 123100: 0.745998\n",
            "loss after iteration 123200: 0.744881\n",
            "loss after iteration 123300: 0.743766\n",
            "loss after iteration 123400: 0.742654\n",
            "loss after iteration 123500: 0.741545\n",
            "loss after iteration 123600: 0.740438\n",
            "loss after iteration 123700: 0.739334\n",
            "loss after iteration 123800: 0.738233\n",
            "loss after iteration 123900: 0.737135\n",
            "loss after iteration 124000: 0.736039\n",
            "loss after iteration 124100: 0.734946\n",
            "loss after iteration 124200: 0.733855\n",
            "loss after iteration 124300: 0.732767\n",
            "loss after iteration 124400: 0.731682\n",
            "loss after iteration 124500: 0.730599\n",
            "loss after iteration 124600: 0.729519\n",
            "loss after iteration 124700: 0.728442\n",
            "loss after iteration 124800: 0.727367\n",
            "loss after iteration 124900: 0.726295\n",
            "loss after iteration 125000: 0.725225\n",
            "loss after iteration 125100: 0.724158\n",
            "loss after iteration 125200: 0.723093\n",
            "loss after iteration 125300: 0.722031\n",
            "loss after iteration 125400: 0.720972\n",
            "loss after iteration 125500: 0.719915\n",
            "loss after iteration 125600: 0.718861\n",
            "loss after iteration 125700: 0.717809\n",
            "loss after iteration 125800: 0.716760\n",
            "loss after iteration 125900: 0.715713\n",
            "loss after iteration 126000: 0.714669\n",
            "loss after iteration 126100: 0.713627\n",
            "loss after iteration 126200: 0.712588\n",
            "loss after iteration 126300: 0.711551\n",
            "loss after iteration 126400: 0.710517\n",
            "loss after iteration 126500: 0.709485\n",
            "loss after iteration 126600: 0.708456\n",
            "loss after iteration 126700: 0.707429\n",
            "loss after iteration 126800: 0.706404\n",
            "loss after iteration 126900: 0.705382\n",
            "loss after iteration 127000: 0.704363\n",
            "loss after iteration 127100: 0.703346\n",
            "loss after iteration 127200: 0.702331\n",
            "loss after iteration 127300: 0.701319\n",
            "loss after iteration 127400: 0.700309\n",
            "loss after iteration 127500: 0.699301\n",
            "loss after iteration 127600: 0.698296\n",
            "loss after iteration 127700: 0.697293\n",
            "loss after iteration 127800: 0.696293\n",
            "loss after iteration 127900: 0.695295\n",
            "loss after iteration 128000: 0.694299\n",
            "loss after iteration 128100: 0.693306\n",
            "loss after iteration 128200: 0.692315\n",
            "loss after iteration 128300: 0.691326\n",
            "loss after iteration 128400: 0.690340\n",
            "loss after iteration 128500: 0.689356\n",
            "loss after iteration 128600: 0.688374\n",
            "loss after iteration 128700: 0.687395\n",
            "loss after iteration 128800: 0.686418\n",
            "loss after iteration 128900: 0.685443\n",
            "loss after iteration 129000: 0.684471\n",
            "loss after iteration 129100: 0.683500\n",
            "loss after iteration 129200: 0.682533\n",
            "loss after iteration 129300: 0.681567\n",
            "loss after iteration 129400: 0.680603\n",
            "loss after iteration 129500: 0.679642\n",
            "loss after iteration 129600: 0.678683\n",
            "loss after iteration 129700: 0.677727\n",
            "loss after iteration 129800: 0.676772\n",
            "loss after iteration 129900: 0.675820\n",
            "loss after iteration 130000: 0.674870\n",
            "loss after iteration 130100: 0.673922\n",
            "loss after iteration 130200: 0.672977\n",
            "loss after iteration 130300: 0.672033\n",
            "loss after iteration 130400: 0.671092\n",
            "loss after iteration 130500: 0.670153\n",
            "loss after iteration 130600: 0.669216\n",
            "loss after iteration 130700: 0.668282\n",
            "loss after iteration 130800: 0.667349\n",
            "loss after iteration 130900: 0.666419\n",
            "loss after iteration 131000: 0.665491\n",
            "loss after iteration 131100: 0.664565\n",
            "loss after iteration 131200: 0.663641\n",
            "loss after iteration 131300: 0.662719\n",
            "loss after iteration 131400: 0.661800\n",
            "loss after iteration 131500: 0.660882\n",
            "loss after iteration 131600: 0.659967\n",
            "loss after iteration 131700: 0.659053\n",
            "loss after iteration 131800: 0.658142\n",
            "loss after iteration 131900: 0.657233\n",
            "loss after iteration 132000: 0.656326\n",
            "loss after iteration 132100: 0.655421\n",
            "loss after iteration 132200: 0.654518\n",
            "loss after iteration 132300: 0.653618\n",
            "loss after iteration 132400: 0.652719\n",
            "loss after iteration 132500: 0.651822\n",
            "loss after iteration 132600: 0.650928\n",
            "loss after iteration 132700: 0.650035\n",
            "loss after iteration 132800: 0.649145\n",
            "loss after iteration 132900: 0.648256\n",
            "loss after iteration 133000: 0.647370\n",
            "loss after iteration 133100: 0.646485\n",
            "loss after iteration 133200: 0.645603\n",
            "loss after iteration 133300: 0.644723\n",
            "loss after iteration 133400: 0.643844\n",
            "loss after iteration 133500: 0.642968\n",
            "loss after iteration 133600: 0.642093\n",
            "loss after iteration 133700: 0.641221\n",
            "loss after iteration 133800: 0.640350\n",
            "loss after iteration 133900: 0.639482\n",
            "loss after iteration 134000: 0.638615\n",
            "loss after iteration 134100: 0.637751\n",
            "loss after iteration 134200: 0.636888\n",
            "loss after iteration 134300: 0.636028\n",
            "loss after iteration 134400: 0.635169\n",
            "loss after iteration 134500: 0.634312\n",
            "loss after iteration 134600: 0.633457\n",
            "loss after iteration 134700: 0.632604\n",
            "loss after iteration 134800: 0.631753\n",
            "loss after iteration 134900: 0.630904\n",
            "loss after iteration 135000: 0.630057\n",
            "loss after iteration 135100: 0.629212\n",
            "loss after iteration 135200: 0.628369\n",
            "loss after iteration 135300: 0.627527\n",
            "loss after iteration 135400: 0.626688\n",
            "loss after iteration 135500: 0.625850\n",
            "loss after iteration 135600: 0.625014\n",
            "loss after iteration 135700: 0.624180\n",
            "loss after iteration 135800: 0.623348\n",
            "loss after iteration 135900: 0.622518\n",
            "loss after iteration 136000: 0.621690\n",
            "loss after iteration 136100: 0.620863\n",
            "loss after iteration 136200: 0.620038\n",
            "loss after iteration 136300: 0.619215\n",
            "loss after iteration 136400: 0.618394\n",
            "loss after iteration 136500: 0.617575\n",
            "loss after iteration 136600: 0.616758\n",
            "loss after iteration 136700: 0.615942\n",
            "loss after iteration 136800: 0.615129\n",
            "loss after iteration 136900: 0.614317\n",
            "loss after iteration 137000: 0.613507\n",
            "loss after iteration 137100: 0.612698\n",
            "loss after iteration 137200: 0.611892\n",
            "loss after iteration 137300: 0.611087\n",
            "loss after iteration 137400: 0.610284\n",
            "loss after iteration 137500: 0.609483\n",
            "loss after iteration 137600: 0.608683\n",
            "loss after iteration 137700: 0.607886\n",
            "loss after iteration 137800: 0.607090\n",
            "loss after iteration 137900: 0.606296\n",
            "loss after iteration 138000: 0.605503\n",
            "loss after iteration 138100: 0.604713\n",
            "loss after iteration 138200: 0.603924\n",
            "loss after iteration 138300: 0.603136\n",
            "loss after iteration 138400: 0.602351\n",
            "loss after iteration 138500: 0.601567\n",
            "loss after iteration 138600: 0.600785\n",
            "loss after iteration 138700: 0.600005\n",
            "loss after iteration 138800: 0.599226\n",
            "loss after iteration 138900: 0.598449\n",
            "loss after iteration 139000: 0.597674\n",
            "loss after iteration 139100: 0.596901\n",
            "loss after iteration 139200: 0.596129\n",
            "loss after iteration 139300: 0.595359\n",
            "loss after iteration 139400: 0.594590\n",
            "loss after iteration 139500: 0.593823\n",
            "loss after iteration 139600: 0.593058\n",
            "loss after iteration 139700: 0.592295\n",
            "loss after iteration 139800: 0.591533\n",
            "loss after iteration 139900: 0.590773\n",
            "loss after iteration 140000: 0.590014\n",
            "loss after iteration 140100: 0.589258\n",
            "loss after iteration 140200: 0.588502\n",
            "loss after iteration 140300: 0.587749\n",
            "loss after iteration 140400: 0.586997\n",
            "loss after iteration 140500: 0.586246\n",
            "loss after iteration 140600: 0.585498\n",
            "loss after iteration 140700: 0.584751\n",
            "loss after iteration 140800: 0.584005\n",
            "loss after iteration 140900: 0.583261\n",
            "loss after iteration 141000: 0.582519\n",
            "loss after iteration 141100: 0.581778\n",
            "loss after iteration 141200: 0.581039\n",
            "loss after iteration 141300: 0.580302\n",
            "loss after iteration 141400: 0.579566\n",
            "loss after iteration 141500: 0.578832\n",
            "loss after iteration 141600: 0.578099\n",
            "loss after iteration 141700: 0.577368\n",
            "loss after iteration 141800: 0.576638\n",
            "loss after iteration 141900: 0.575910\n",
            "loss after iteration 142000: 0.575184\n",
            "loss after iteration 142100: 0.574459\n",
            "loss after iteration 142200: 0.573735\n",
            "loss after iteration 142300: 0.573013\n",
            "loss after iteration 142400: 0.572293\n",
            "loss after iteration 142500: 0.571574\n",
            "loss after iteration 142600: 0.570857\n",
            "loss after iteration 142700: 0.570141\n",
            "loss after iteration 142800: 0.569427\n",
            "loss after iteration 142900: 0.568715\n",
            "loss after iteration 143000: 0.568003\n",
            "loss after iteration 143100: 0.567294\n",
            "loss after iteration 143200: 0.566586\n",
            "loss after iteration 143300: 0.565879\n",
            "loss after iteration 143400: 0.565174\n",
            "loss after iteration 143500: 0.564470\n",
            "loss after iteration 143600: 0.563768\n",
            "loss after iteration 143700: 0.563067\n",
            "loss after iteration 143800: 0.562368\n",
            "loss after iteration 143900: 0.561670\n",
            "loss after iteration 144000: 0.560974\n",
            "loss after iteration 144100: 0.560279\n",
            "loss after iteration 144200: 0.559586\n",
            "loss after iteration 144300: 0.558894\n",
            "loss after iteration 144400: 0.558204\n",
            "loss after iteration 144500: 0.557515\n",
            "loss after iteration 144600: 0.556827\n",
            "loss after iteration 144700: 0.556141\n",
            "loss after iteration 144800: 0.555457\n",
            "loss after iteration 144900: 0.554773\n",
            "loss after iteration 145000: 0.554092\n",
            "loss after iteration 145100: 0.553411\n",
            "loss after iteration 145200: 0.552732\n",
            "loss after iteration 145300: 0.552055\n",
            "loss after iteration 145400: 0.551379\n",
            "loss after iteration 145500: 0.550704\n",
            "loss after iteration 145600: 0.550031\n",
            "loss after iteration 145700: 0.549359\n",
            "loss after iteration 145800: 0.548689\n",
            "loss after iteration 145900: 0.548019\n",
            "loss after iteration 146000: 0.547352\n",
            "loss after iteration 146100: 0.546685\n",
            "loss after iteration 146200: 0.546021\n",
            "loss after iteration 146300: 0.545357\n",
            "loss after iteration 146400: 0.544695\n",
            "loss after iteration 146500: 0.544034\n",
            "loss after iteration 146600: 0.543375\n",
            "loss after iteration 146700: 0.542717\n",
            "loss after iteration 146800: 0.542060\n",
            "loss after iteration 146900: 0.541404\n",
            "loss after iteration 147000: 0.540750\n",
            "loss after iteration 147100: 0.540098\n",
            "loss after iteration 147200: 0.539447\n",
            "loss after iteration 147300: 0.538797\n",
            "loss after iteration 147400: 0.538148\n",
            "loss after iteration 147500: 0.537501\n",
            "loss after iteration 147600: 0.536855\n",
            "loss after iteration 147700: 0.536210\n",
            "loss after iteration 147800: 0.535567\n",
            "loss after iteration 147900: 0.534925\n",
            "loss after iteration 148000: 0.534284\n",
            "loss after iteration 148100: 0.533645\n",
            "loss after iteration 148200: 0.533007\n",
            "loss after iteration 148300: 0.532370\n",
            "loss after iteration 148400: 0.531734\n",
            "loss after iteration 148500: 0.531100\n",
            "loss after iteration 148600: 0.530467\n",
            "loss after iteration 148700: 0.529836\n",
            "loss after iteration 148800: 0.529206\n",
            "loss after iteration 148900: 0.528577\n",
            "loss after iteration 149000: 0.527949\n",
            "loss after iteration 149100: 0.527322\n",
            "loss after iteration 149200: 0.526697\n",
            "loss after iteration 149300: 0.526073\n",
            "loss after iteration 149400: 0.525451\n",
            "loss after iteration 149500: 0.524829\n",
            "loss after iteration 149600: 0.524209\n",
            "loss after iteration 149700: 0.523590\n",
            "loss after iteration 149800: 0.522973\n",
            "loss after iteration 149900: 0.522356\n"
          ]
        }
      ],
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters =  NeuralNetwork(trainx, trainy,10, num_iterations = 150000, learning_rate = 0.01, print_loss=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8vFWiwz3FWA"
      },
      "source": [
        "## Check the accuracy on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BUdvhN9BoXDn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, trainx)\n",
        "print(accuracy_score(Y.T, predictions.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjg3DINw3FWB"
      },
      "source": [
        "## Check the accuracy on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2-pHP3ts3FWB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "predictions_test = predict(parameters,testx)\n",
        "print(accuracy_score(testy.T, predictions_test.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrT2xrXeoXDz"
      },
      "source": [
        "## Run the model multiple times with different hyperparameters and write your interpretation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66OLHmwvoXD0"
      },
      "source": [
        "What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvoglA35oXD3"
      },
      "source": [
        "References:\n",
        "- https://www.coursera.org/learn/neural-networks-deep-learning\n",
        "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
        "- http://cs231n.github.io/neural-networks-case-study/\n",
        "- https://archive.ics.uci.edu/ml/datasets.php"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Neural Network From Scratch.ipynb",
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "wRuwL",
      "launcher_item_id": "NI888"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "7037ec29b621de18a3396a0455ec45ad55ce896f0aa235683271d94d3867e1de"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
