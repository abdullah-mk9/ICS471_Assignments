{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create encoder\n",
    "I will use the pretrained VGG16 for the encoding part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "# import required module\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "from keras.layers import (LSTM, Embedding, \n",
    "    TimeDistributed, Dense, RepeatVector, \n",
    "    Activation, Flatten, Reshape, concatenate,  \n",
    "    Dropout, BatchNormalization)\n",
    "\n",
    "# assign directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "from keras.applications import MobileNet\n",
    "import keras.applications.mobilenet  \n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import keras.applications.inception_v3\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import keras.preprocessing.image\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (LSTM, Embedding, \n",
    "    TimeDistributed, Dense, RepeatVector, \n",
    "    Activation, Flatten, Reshape, concatenate,  \n",
    "    Dropout, BatchNormalization)\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import add\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate the groundTruth.txt to english\n",
    "I decided to go with english given my expreience with existing arabic `word2vec` models. <br>\n",
    "For the embeddings I will use spaCy by which yields 300 words embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>god name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thank god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all deaf hearing arabs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peace be upon you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>today i present to you another programme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the subject of the study of arabic sign language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>today words are sparse in religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>also ordinary words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>no partner of god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>allah is the greatest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "1                                           god name\n",
       "2                                          thank god\n",
       "3                             all deaf hearing arabs\n",
       "4                                  peace be upon you\n",
       "5           today i present to you another programme\n",
       "6   the subject of the study of arabic sign language\n",
       "7                 today words are sparse in religion\n",
       "8                                also ordinary words\n",
       "9                                  no partner of god\n",
       "10                             allah is the greatest"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captionlist = {\n",
    "    1: \"god name\",\n",
    "    2: \"thank god\",\n",
    "    3: \"all deaf hearing arabs\",\n",
    "    4: \"peace be upon you\",\n",
    "    5: \"today i present to you another programme\",\n",
    "    6: \"the subject of the study of arabic sign language\",\n",
    "    7: \"today words are sparse in religion\",\n",
    "    8: \"also ordinary words\",\n",
    "    9: \"no partner of god\",\n",
    "    10: \"allah is the greatest\"\n",
    "\n",
    "}\n",
    "\n",
    "captions = pd.DataFrame.from_dict(captionlist, orient='index')\n",
    "# change the index to be in range 1 - 10\n",
    "captions.index = [i+1 for i in range(10)]\n",
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = len(max(captions[0], key=len).split(\" \"))\n",
    "VOCAB = set(\" \".join(captions[0]).split(\" \"))\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "IMG_SIZE = 224\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EMBEDDING_SHAPE = 300\n",
    "OUTPUT_DIM = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the start and end tokens for each caption <br>\n",
    "`startseq`  {Caption}   `endseq `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>startseq god name endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>startseq peace be upon you endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>startseq today i present to you another progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>startseq the subject of the study of arabic si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>startseq today words are sparse in religion en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>startseq also ordinary words endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>startseq no partner of god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>startseq allah is the greatest endseq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "1                            startseq god name endseq\n",
       "2                           startseq thank god endseq\n",
       "3              startseq all deaf hearing arabs endseq\n",
       "4                   startseq peace be upon you endseq\n",
       "5   startseq today i present to you another progra...\n",
       "6   startseq the subject of the study of arabic si...\n",
       "7   startseq today words are sparse in religion en...\n",
       "8                 startseq also ordinary words endseq\n",
       "9                   startseq no partner of god endseq\n",
       "10              startseq allah is the greatest endseq"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    captions[0][i+1] = START + \" \" + captions[0][i+1] + \" \" + STOP\n",
    "\n",
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Only dirs are loaded. This is done to avoid the heavy load on memeory, the data will be dynamically loaded through some utilites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>image_dir</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_name  \\\n",
       "0  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "1  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "2  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "3  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "4  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "\n",
       "                                           image_dir  \\\n",
       "0  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "1  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "2  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "3  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "4  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "\n",
       "                                  caption  \n",
       "0  startseq all deaf hearing arabs endseq  \n",
       "1  startseq all deaf hearing arabs endseq  \n",
       "2  startseq all deaf hearing arabs endseq  \n",
       "3  startseq all deaf hearing arabs endseq  \n",
       "4  startseq all deaf hearing arabs endseq  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "directory = '../data/train'\n",
    "\n",
    "videos = []\n",
    "images_dir = []\n",
    "labels = []\n",
    "df = pd.DataFrame()\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for label in os.listdir(directory):\n",
    "    f1 = os.path.join(directory, label)\n",
    "    for video in os.listdir(f1):\n",
    "        f2 = os.path.join(f1, video)\n",
    "        for frame in os.listdir(f2):\n",
    "            videos.append(f2)\n",
    "            images_dir.append(os.path.join(f2, frame))\n",
    "            labels.append(int(label))\n",
    "\n",
    "df['video_name'] = videos\n",
    "df['image_dir'] = images_dir\n",
    "df['caption'] = labels\n",
    "df['caption'] = df['caption'].apply(lambda x: captions[0][x])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    InceptionV3_model = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    # preprocess the input\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input(inputs)\n",
    "\n",
    "    # extract the features from the preprocessed input\n",
    "    outputs = InceptionV3_model(preprocess_input)\n",
    "\n",
    "    # form the final model \n",
    "    myModel = keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "    \n",
    "    return myModel\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utilites for data reading and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_reader(img_paths):\n",
    "    \"\"\"\n",
    "    Takes and array of paths and read and resize the them.\n",
    "    This is for dynamic reading\n",
    "    \"\"\"\n",
    "    if not isinstance(img_paths, list):\n",
    "        img_paths = [img_paths]\n",
    "    images = []\n",
    "    for path in img_paths:\n",
    "        image = plt.imread(path)\n",
    "        image = resize_image(image)\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "def resize_image(image):\n",
    "    \"\"\"\n",
    "    Resize images to a desired shape, does not account for aspect ratio \n",
    "    \"\"\"\n",
    "    _,_,depth = image.shape\n",
    "    return np.resize(image, (IMG_SIZE, IMG_SIZE, depth))\n",
    "\n",
    "def gather_video_frames(video_path, df):\n",
    "    \"\"\"\n",
    "    Returns all dirs of frames that belong to a video\n",
    "    \"\"\"\n",
    "    frames = df[df['video_name'] == video_path]['image_dir'].values.tolist()\n",
    "    return frames\n",
    "    \n",
    "# def prepare_all_videos(df):\n",
    "#     video_paths = np.unique(df['video_name']).tolist()\n",
    "#     captions = [df[df['video_name'] == i]['caption'] for i in video_paths]\n",
    "\n",
    "#     allVideosFeatures = []\n",
    "#     for idx, path in enumerate(video_paths):\n",
    "#         frames = image_reader(gather_video_frames(path, df))\n",
    "#         videoFeatures = feature_extractor.predict(frames, verbose=0)\n",
    "#         allVideosFeatures.append(videoFeatures.squeeze())\n",
    "\n",
    "#     return np.array(allVideosFeatures), captions\n",
    "    \n",
    "def prepare_single_video(df, path):\n",
    "    \"\"\"\n",
    "    Takes a video and encode all its frames and return the encoded frames and the captions\n",
    "    The backbone of the generator as it allows dynamic reading of the data\n",
    "    \"\"\"\n",
    "    caption = df[df['video_name'] == path]['caption'].values.tolist()[0]\n",
    "    videoFeatures = []\n",
    "    frames = image_reader(gather_video_frames(path,df))\n",
    "    features = feature_extractor.predict(frames, verbose=0)\n",
    "    videoFeatures.append(features.squeeze())\n",
    "\n",
    "    return np.squeeze(np.array(videoFeatures)), caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word dictionaries and embeddings\n",
    "Two dictionaries are defined:\n",
    "* `idxtoword`: assists in the prediction \n",
    "* `wordtoidx`: to set up the labels correctly for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 name\n",
      "1 the\n",
      "2 partner\n",
      "3 another\n",
      "4 upon\n",
      "5 also\n",
      "6 allah\n",
      "7 be\n",
      "8 programme\n",
      "9 is\n",
      "10 thank\n",
      "11 all\n",
      "12 hearing\n",
      "13 of\n",
      "14 are\n",
      "15 to\n",
      "16 religion\n",
      "17 no\n",
      "18 words\n",
      "19 present\n",
      "20 sparse\n",
      "21 ordinary\n",
      "22 sign\n",
      "23 peace\n",
      "24 you\n",
      "25 today\n",
      "26 subject\n",
      "27 study\n",
      "28 god\n",
      "29 language\n",
      "30 deaf\n",
      "31 i\n",
      "32 greatest\n",
      "33 arabic\n",
      "34 in\n",
      "35 arabs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_SHAPE))\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    return nlp(word).vector\n",
    "\n",
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "\n",
    "for i,w in enumerate(VOCAB):\n",
    "    print(i,w)\n",
    "    wordtoidx[w] = i\n",
    "    idxtoword[i] = w\n",
    "    embedding_matrix[i] = get_word_embedding(w)\n",
    "\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df, wordtoidx, \\\n",
    "                    batch_size):\n",
    "  # x1 - Training data for df\n",
    "  # x2 - The caption that goes with each photo\n",
    "  # y - The predicted rest of the caption\n",
    "  x1, x2, y = [], [], []\n",
    "  n=0\n",
    "\n",
    "  videos = np.unique(df['video_name']).tolist()\n",
    "\n",
    "  while True:\n",
    "    for video in videos:\n",
    "      n+=1\n",
    "      hidden_state, caption = prepare_single_video(df, video)\n",
    "      # Convert each word into a list of sequences.\n",
    "      seq = [wordtoidx[word] for word in caption.split(' ') \\\n",
    "               if word in wordtoidx]\n",
    "      # Generate a training case for every possible sequence and outcome\n",
    "      for i in range(1, len(seq)):\n",
    "          in_seq, out_seq = seq[:i], seq[i]\n",
    "          in_seq = pad_sequences([in_seq], maxlen=MAX_SEQ_LENGTH)[0]\n",
    "          out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n",
    "          x1.append(hidden_state)\n",
    "          x2.append(in_seq)\n",
    "          y.append(out_seq)\n",
    "      if n==batch_size:\n",
    "        yield ([np.array(x1)[0], np.array(x2)], np.array(y))\n",
    "        x1, x2,y = [], [], []\n",
    "        n=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "inputs1 = keras.Input(shape=(80, OUTPUT_DIM))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = LSTM(256, activation='relu')(fe1)\n",
    "inputs2 = keras.Input(shape=(MAX_SEQ_LENGTH,))\n",
    "se1 = Embedding(VOCAB_SIZE, EMBEDDING_SHAPE, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(VOCAB_SIZE, activation='softmax')(decoder2)\n",
    "caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "caption_model.layers[2].set_weights([embedding_matrix])\n",
    "caption_model.layers[2].trainable = False\n",
    "adamOptimizer = keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer=adamOptimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 80, 2048)]   0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 9, 300)       10800       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 80, 2048)     0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 9, 300)       0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          2360320     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 256)          570368      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['lstm[0][0]',                   \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 36)           9252        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,016,532\n",
      "Trainable params: 3,005,732\n",
      "Non-trainable params: 10,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 14:15:02.422046: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 80, 2048) for input KerasTensor(type_spec=TensorSpec(shape=(None, 80, 2048), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None, None).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"model\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received by layer \"model\" \"                 f\"(type Functional):\n      • inputs=('tf.Tensor(shape=(None, None), dtype=float32)', 'tf.Tensor(shape=(None, 9), dtype=int32)')\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m earlyStopper \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)     \n\u001b[1;32m      5\u001b[0m generator \u001b[39m=\u001b[39m data_generator(df, wordtoidx, \u001b[39m80\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m caption_model\u001b[39m.\u001b[39;49mfit(generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m                   verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[checkpoint, earlyStopper])\n",
      "File \u001b[0;32m~/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/7n/ynz1z7qd3fbg7568r_zgsb980000gn/T/__autograph_generated_filezlz1twud.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/waleedalasad/Documents/GitHub/ICS471_Assignments/venv/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"model\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, None)\n    \n    Call arguments received by layer \"model\" \"                 f\"(type Functional):\n      • inputs=('tf.Tensor(shape=(None, None), dtype=float32)', 'tf.Tensor(shape=(None, 9), dtype=int32)')\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        '../model', save_weights_only=True, save_best_only=True, verbose=1 )\n",
    "earlyStopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)     \n",
    "\n",
    "generator = data_generator(df, wordtoidx, 80)\n",
    "caption_model.fit(generator, epochs=10,\n",
    "                  verbose=1, callbacks=[checkpoint, earlyStopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.695311365857272, 16.11809565095832)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_prime(n):\n",
    "  for i in range(2,n):\n",
    "    if (n%i) == 0:\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "\n",
    "i = 1\n",
    "a = []\n",
    "for j in range(10000000):\n",
    "    a.append(1/(j+1) * i)\n",
    "    # i = i * -1\n",
    "\n",
    "sum(a),np.log(10000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
