{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create encoder\n",
    "I will use the pretrained VGG16 for the encoding part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "# import required module\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "from keras.layers import (LSTM, Embedding, \n",
    "    TimeDistributed, Dense, RepeatVector, \n",
    "    Activation, Flatten, Reshape, concatenate,  \n",
    "    Dropout, BatchNormalization)\n",
    "\n",
    "# assign directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "from keras.applications import MobileNet\n",
    "import keras.applications.mobilenet  \n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import keras.applications.inception_v3\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import keras.preprocessing.image\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (LSTM, Embedding, \n",
    "    TimeDistributed, Dense, RepeatVector, \n",
    "    Activation, Flatten, Reshape, concatenate,  \n",
    "    Dropout, BatchNormalization)\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import add\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate the groundTruth.txt to english\n",
    "I decided to go with english given my expreience with existing arabic `word2vec` models. <br>\n",
    "For the embeddings I will use spaCy by which yields 300 words embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>god name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thank god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all deaf hearing arabs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peace be upon you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>today i present to you another programme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the subject of the study of arabic sign language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>today words are sparse in religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>also ordinary words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>no partner of god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>allah is the greatest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "1                                           god name\n",
       "2                                          thank god\n",
       "3                             all deaf hearing arabs\n",
       "4                                  peace be upon you\n",
       "5           today i present to you another programme\n",
       "6   the subject of the study of arabic sign language\n",
       "7                 today words are sparse in religion\n",
       "8                                also ordinary words\n",
       "9                                  no partner of god\n",
       "10                             allah is the greatest"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captionlist = {\n",
    "    1: \"god name\",\n",
    "    2: \"thank god\",\n",
    "    3: \"all deaf hearing arabs\",\n",
    "    4: \"peace be upon you\",\n",
    "    5: \"today i present to you another programme\",\n",
    "    6: \"the subject of the study of arabic sign language\",\n",
    "    7: \"today words are sparse in religion\",\n",
    "    8: \"also ordinary words\",\n",
    "    9: \"no partner of god\",\n",
    "    10: \"allah is the greatest\"\n",
    "\n",
    "}\n",
    "\n",
    "captions = pd.DataFrame.from_dict(captionlist, orient='index')\n",
    "# change the index to be in range 1 - 10\n",
    "captions.index = [i+1 for i in range(10)]\n",
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = len(max(captions[0], key=len).split(\" \"))\n",
    "VOCAB = set(\" \".join(captions[0]).split(\" \"))\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "IMG_SIZE = 224\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EMBEDDING_SHAPE = 300\n",
    "OUTPUT_DIM = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the start and end tokens for each caption <br>\n",
    "`startseq`  {Caption}   `endseq `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>startseq god name endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>startseq peace be upon you endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>startseq today i present to you another progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>startseq the subject of the study of arabic si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>startseq today words are sparse in religion en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>startseq also ordinary words endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>startseq no partner of god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>startseq allah is the greatest endseq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "1                            startseq god name endseq\n",
       "2                           startseq thank god endseq\n",
       "3              startseq all deaf hearing arabs endseq\n",
       "4                   startseq peace be upon you endseq\n",
       "5   startseq today i present to you another progra...\n",
       "6   startseq the subject of the study of arabic si...\n",
       "7   startseq today words are sparse in religion en...\n",
       "8                 startseq also ordinary words endseq\n",
       "9                   startseq no partner of god endseq\n",
       "10              startseq allah is the greatest endseq"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    captions[0][i+1] = START + \" \" + captions[0][i+1] + \" \" + STOP\n",
    "\n",
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Only dirs are loaded. This is done to avoid the heavy load on memeory, the data will be dynamically loaded through some utilites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>image_dir</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)_c</td>\n",
       "      <td>../data/train/0003/01_0003_(10_03_21_21_04_26)...</td>\n",
       "      <td>startseq all deaf hearing arabs endseq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_name  \\\n",
       "0  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "1  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "2  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "3  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "4  ../data/train/0003/01_0003_(10_03_21_21_04_26)_c   \n",
       "\n",
       "                                           image_dir  \\\n",
       "0  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "1  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "2  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "3  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "4  ../data/train/0003/01_0003_(10_03_21_21_04_26)...   \n",
       "\n",
       "                                  caption  \n",
       "0  startseq all deaf hearing arabs endseq  \n",
       "1  startseq all deaf hearing arabs endseq  \n",
       "2  startseq all deaf hearing arabs endseq  \n",
       "3  startseq all deaf hearing arabs endseq  \n",
       "4  startseq all deaf hearing arabs endseq  "
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "directory = '../data/train'\n",
    "\n",
    "videos = []\n",
    "images_dir = []\n",
    "labels = []\n",
    "df = pd.DataFrame()\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for label in os.listdir(directory):\n",
    "    f1 = os.path.join(directory, label)\n",
    "    for video in os.listdir(f1):\n",
    "        f2 = os.path.join(f1, video)\n",
    "        for frame in os.listdir(f2):\n",
    "            videos.append(f2)\n",
    "            images_dir.append(os.path.join(f2, frame))\n",
    "            labels.append(int(label))\n",
    "\n",
    "df['video_name'] = videos\n",
    "df['image_dir'] = images_dir\n",
    "df['caption'] = labels\n",
    "df['caption'] = df['caption'].apply(lambda x: captions[0][x])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>image_dir</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12480</th>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12484</th>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(10_03_21_20_53_47)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16475</th>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16476</th>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16477</th>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16478</th>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16479</th>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)_c</td>\n",
       "      <td>../data/train/0002/01_0002_(13_02_21_19_45_29)...</td>\n",
       "      <td>startseq thank god endseq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             video_name  \\\n",
       "12480  ../data/train/0002/01_0002_(10_03_21_20_53_47)_c   \n",
       "12481  ../data/train/0002/01_0002_(10_03_21_20_53_47)_c   \n",
       "12482  ../data/train/0002/01_0002_(10_03_21_20_53_47)_c   \n",
       "12483  ../data/train/0002/01_0002_(10_03_21_20_53_47)_c   \n",
       "12484  ../data/train/0002/01_0002_(10_03_21_20_53_47)_c   \n",
       "...                                                 ...   \n",
       "16475  ../data/train/0002/01_0002_(13_02_21_19_45_29)_c   \n",
       "16476  ../data/train/0002/01_0002_(13_02_21_19_45_29)_c   \n",
       "16477  ../data/train/0002/01_0002_(13_02_21_19_45_29)_c   \n",
       "16478  ../data/train/0002/01_0002_(13_02_21_19_45_29)_c   \n",
       "16479  ../data/train/0002/01_0002_(13_02_21_19_45_29)_c   \n",
       "\n",
       "                                               image_dir  \\\n",
       "12480  ../data/train/0002/01_0002_(10_03_21_20_53_47)...   \n",
       "12481  ../data/train/0002/01_0002_(10_03_21_20_53_47)...   \n",
       "12482  ../data/train/0002/01_0002_(10_03_21_20_53_47)...   \n",
       "12483  ../data/train/0002/01_0002_(10_03_21_20_53_47)...   \n",
       "12484  ../data/train/0002/01_0002_(10_03_21_20_53_47)...   \n",
       "...                                                  ...   \n",
       "16475  ../data/train/0002/01_0002_(13_02_21_19_45_29)...   \n",
       "16476  ../data/train/0002/01_0002_(13_02_21_19_45_29)...   \n",
       "16477  ../data/train/0002/01_0002_(13_02_21_19_45_29)...   \n",
       "16478  ../data/train/0002/01_0002_(13_02_21_19_45_29)...   \n",
       "16479  ../data/train/0002/01_0002_(13_02_21_19_45_29)...   \n",
       "\n",
       "                         caption  \n",
       "12480  startseq thank god endseq  \n",
       "12481  startseq thank god endseq  \n",
       "12482  startseq thank god endseq  \n",
       "12483  startseq thank god endseq  \n",
       "12484  startseq thank god endseq  \n",
       "...                          ...  \n",
       "16475  startseq thank god endseq  \n",
       "16476  startseq thank god endseq  \n",
       "16477  startseq thank god endseq  \n",
       "16478  startseq thank god endseq  \n",
       "16479  startseq thank god endseq  \n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['caption'] == 'startseq thank god endseq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    InceptionV3_model = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    # preprocess the input\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input(inputs)\n",
    "\n",
    "    # extract the features from the preprocessed input\n",
    "    outputs = InceptionV3_model(preprocess_input)\n",
    "\n",
    "    # form the final model \n",
    "    myModel = keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "    \n",
    "    return myModel\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utilites for data reading and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_reader(img_paths):\n",
    "    \"\"\"\n",
    "    Takes and array of paths and read and resize the them.\n",
    "    This is for dynamic reading\n",
    "    \"\"\"\n",
    "    if not isinstance(img_paths, list):\n",
    "        img_paths = [img_paths]\n",
    "    images = []\n",
    "    for path in img_paths:\n",
    "        image = plt.imread(path)\n",
    "        image = resize_image(image)\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "def resize_image(image):\n",
    "    \"\"\"\n",
    "    Resize images to a desired shape, does not account for aspect ratio \n",
    "    \"\"\"\n",
    "    _,_,depth = image.shape\n",
    "    return np.resize(image, (IMG_SIZE, IMG_SIZE, depth))\n",
    "\n",
    "def gather_video_frames(video_path, df):\n",
    "    \"\"\"\n",
    "    Returns all dirs of frames that belong to a video\n",
    "    \"\"\"\n",
    "    frames = df[df['video_name'] == video_path]['image_dir'].values.tolist()\n",
    "    return frames\n",
    "    \n",
    "# def prepare_all_videos(df):\n",
    "#     video_paths = np.unique(df['video_name']).tolist()\n",
    "#     captions = [df[df['video_name'] == i]['caption'] for i in video_paths]\n",
    "\n",
    "#     allVideosFeatures = []\n",
    "#     for idx, path in enumerate(video_paths):\n",
    "#         frames = image_reader(gather_video_frames(path, df))\n",
    "#         videoFeatures = feature_extractor.predict(frames, verbose=0)\n",
    "#         allVideosFeatures.append(videoFeatures.squeeze())\n",
    "\n",
    "#     return np.array(allVideosFeatures), captions\n",
    "    \n",
    "def prepare_single_video(df, path):\n",
    "    \"\"\"\n",
    "    Takes a video and encode all its frames and return the encoded frames and the captions\n",
    "    The backbone of the generator as it allows dynamic reading of the data\n",
    "    \"\"\"\n",
    "    caption = df[df['video_name'] == path]['caption'].values.tolist()[0]\n",
    "    videoFeatures = []\n",
    "    frames = image_reader(gather_video_frames(path,df))\n",
    "    features = feature_extractor.predict(frames, verbose=0)\n",
    "    videoFeatures.append(features.squeeze())\n",
    "\n",
    "    return np.squeeze(np.array(videoFeatures)), caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word dictionaries and embeddings\n",
    "Two dictionaries are defined:\n",
    "* `idxtoword`: assists in the prediction \n",
    "* `wordtoidx`: to set up the labels correctly for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 peace\n",
      "1 to\n",
      "2 religion\n",
      "3 ordinary\n",
      "4 be\n",
      "5 allah\n",
      "6 sign\n",
      "7 deaf\n",
      "8 are\n",
      "9 present\n",
      "10 subject\n",
      "11 in\n",
      "12 sparse\n",
      "13 no\n",
      "14 another\n",
      "15 today\n",
      "16 words\n",
      "17 programme\n",
      "18 is\n",
      "19 the\n",
      "20 also\n",
      "21 upon\n",
      "22 all\n",
      "23 hearing\n",
      "24 arabic\n",
      "25 thank\n",
      "26 i\n",
      "27 language\n",
      "28 of\n",
      "29 god\n",
      "30 name\n",
      "31 greatest\n",
      "32 partner\n",
      "33 study\n",
      "34 arabs\n",
      "35 you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36, 300)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_SHAPE))\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    return nlp(word).vector\n",
    "\n",
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "\n",
    "for i,w in enumerate(VOCAB):\n",
    "    print(i,w)\n",
    "    wordtoidx[w] = i\n",
    "    idxtoword[i] = w\n",
    "    embedding_matrix[i] = get_word_embedding(w)\n",
    "\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df, wordtoidx, \\\n",
    "                    batch_size):\n",
    "  # x1 - Training data for df\n",
    "  # x2 - The caption that goes with each photo\n",
    "  # y - The predicted rest of the caption\n",
    "  x1, x2, y = [], [], []\n",
    "  n=0\n",
    "\n",
    "  videos = np.unique(df['video_name']).tolist()\n",
    "\n",
    "  while True:\n",
    "    for video in videos:\n",
    "      n+=1\n",
    "      hidden_state, caption = prepare_single_video(df, '../data/train/0002/01_0002_(10_03_21_20_53_47)_c')\n",
    "      # Convert each word into a list of sequences.\n",
    "      seq = [wordtoidx[word] for word in caption.split(' ') \\\n",
    "               if word in wordtoidx]\n",
    "      # Generate a training case for every possible sequence and outcome\n",
    "      for i in range(1, len(seq)):\n",
    "          in_seq, out_seq = seq[:i], seq[i]\n",
    "          in_seq = pad_sequences([in_seq], maxlen=MAX_SEQ_LENGTH)[0]\n",
    "          out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n",
    "          x1.append(hidden_state)\n",
    "          x2.append(in_seq)\n",
    "          y.append(out_seq)\n",
    "      if n==batch_size:\n",
    "        yield ([np.array(x1)[0], np.array(x2)], np.array(y))\n",
    "        x1, x2,y = [], [], []\n",
    "        n=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "inputs1 = keras.Input(shape=(OUTPUT_DIM,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = keras.Input(shape=(MAX_SEQ_LENGTH,))\n",
    "se1 = Embedding(VOCAB_SIZE, EMBEDDING_SHAPE, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(VOCAB_SIZE, activation='softmax')(decoder2)\n",
    "caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "caption_model.layers[2].set_weights([embedding_matrix])\n",
    "caption_model.layers[2].trainable = False\n",
    "adamOptimizer = keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer=adamOptimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_46 (InputLayer)          [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " input_45 (InputLayer)          [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 9, 300)       10800       ['input_46[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 2048)         0           ['input_45[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 9, 300)       0           ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 256)          524544      ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  (None, 256)          570368      ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 256)          0           ['dense_21[0][0]',               \n",
      "                                                                  'lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 256)          65792       ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 36)           9252        ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,180,756\n",
      "Trainable params: 1,169,956\n",
      "Non-trainable params: 10,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "     32/Unknown - 4293s 138s/step - loss: 0.8002 - accuracy: 0.8578"
     ]
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        '../model', save_weights_only=True, save_best_only=True, verbose=1 )\n",
    "earlyStopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)     \n",
    "\n",
    "generator = data_generator(df, wordtoidx, 80)\n",
    "caption_model.fit(generator, epochs=10,\n",
    "                  verbose=1, callbacks=[checkpoint, earlyStopper])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0efaa9bf881ecf628eb514d2eb44f95462bf7b4652cfbe658bbcdccb5f2245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
